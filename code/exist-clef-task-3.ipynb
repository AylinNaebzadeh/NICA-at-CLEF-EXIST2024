{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8242355,"sourceType":"datasetVersion","datasetId":4889218}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-27T14:16:43.470647Z","iopub.execute_input":"2024-04-27T14:16:43.470981Z","iopub.status.idle":"2024-04-27T14:16:44.570233Z","shell.execute_reply.started":"2024-04-27T14:16:43.470954Z","shell.execute_reply":"2024-04-27T14:16:44.569120Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/exist-2024-clef/train_data_task1.csv\n/kaggle/input/exist-2024-clef/dev_data_task3.csv\n/kaggle/input/exist-2024-clef/train_data_task2.csv\n/kaggle/input/exist-2024-clef/dev_data_task1.csv\n/kaggle/input/exist-2024-clef/train_data_task3.csv\n/kaggle/input/exist-2024-clef/EXIT2024_tweet_test.csv\n/kaggle/input/exist-2024-clef/dev_data_task2.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb offline\n!wandb disabled","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:17:36.796789Z","iopub.execute_input":"2024-04-27T14:17:36.797843Z","iopub.status.idle":"2024-04-27T14:17:40.874512Z","shell.execute_reply.started":"2024-04-27T14:17:36.797811Z","shell.execute_reply":"2024-04-27T14:17:40.873227Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\nW&B disabled.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:17:46.180734Z","iopub.execute_input":"2024-04-27T14:17:46.181724Z","iopub.status.idle":"2024-04-27T14:18:12.147153Z","shell.execute_reply.started":"2024-04-27T14:17:46.181662Z","shell.execute_reply":"2024-04-27T14:18:12.146201Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from  IPython.display import clear_output\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport pandas as pd\nimport numpy as np\nimport random\nimport  matplotlib. pyplot  as  plt\nfrom tqdm import tqdm\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:18:22.219139Z","iopub.execute_input":"2024-04-27T14:18:22.220056Z","iopub.status.idle":"2024-04-27T14:18:42.507730Z","shell.execute_reply.started":"2024-04-27T14:18:22.220020Z","shell.execute_reply":"2024-04-27T14:18:42.506893Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-04-27 14:18:32.723464: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-27 14:18:32.723573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-27 14:18:32.897921: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/exist-2024-clef/train_data_task3.csv')\ndev_df = pd.read_csv('/kaggle/input/exist-2024-clef/dev_data_task3.csv')\ntest_df = pd.read_csv('/kaggle/input/exist-2024-clef/EXIT2024_tweet_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:58:13.188762Z","iopub.execute_input":"2024-04-27T14:58:13.189415Z","iopub.status.idle":"2024-04-27T14:58:13.312598Z","shell.execute_reply.started":"2024-04-27T14:58:13.189384Z","shell.execute_reply":"2024-04-27T14:58:13.311812Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef print_custom(text):\n    print('\\n')\n    print(text)\n    print('-'*100)\n\n    \ndef simple_preprocess(text):\n    \"\"\"\n    pass the tweet data as a series. do not use apply function\n    only preprocesses for replacing @USER and URLS\n    \"\"\"\n    # print(\"i am preprocessing\")\n    URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n    HANDLE_RE = re.compile(r\"@\\w+\")\n    tweets = []\n    for t in text:\n        t = HANDLE_RE.sub(\"@USER\", t)\n        t = URL_RE.sub(\"HTTPURL\", t)\n        tweets.append(t)\n    return tweets","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:22:50.149929Z","iopub.execute_input":"2024-04-27T14:22:50.150343Z","iopub.status.idle":"2024-04-27T14:22:50.157786Z","shell.execute_reply.started":"2024-04-27T14:22:50.150314Z","shell.execute_reply":"2024-04-27T14:22:50.156593Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\n# Instantiate multi-label encoder\ntask_encoder = MultiLabelBinarizer()\n\n   \ndef task_hard_encode(df):\n    import numpy as np\n    # Create a new DataFrame to store the transformed labels\n    transformed_df = df.copy()\n    task_encoder.fit(all_task_hard_labels)\n    all_labels = []\n    for i in range(len(df['hard_label'])):\n        # Apply eval to each element of the label column\n        all_labels.append(eval(df['hard_label'][i]))\n    \n    transformed_labels = task_encoder.transform(all_labels)\n    # print(pd.DataFrame(transformed_labels))\n    # transformed_df[\"hard_label\"] = pd.DataFrame(transformed_labels)\n    transformed_labels = pd.Series([x.tolist() for x in transformed_labels])\n    transformed_df[\"hard_label\"] = transformed_labels\n    return transformed_df\n\ndef task_hard_decode(df):\n    # Inverse transform the binary-encoded vectors back to original labels\n    decoded_df = df.copy()\n    decode_labels = np.array(decoded_df[\"hard_label\"].tolist())\n\n    # Inverse transform the label column using the provided task_encoder\n    decoded_labels = task_encoder.inverse_transform(decode_labels)\n    \n    # Assign the decoded labels back to the DataFrame\n    decoded_df[\"hard_labels\"] = decoded_labels\n\n    return decoded_df","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:23:00.826669Z","iopub.execute_input":"2024-04-27T14:23:00.827567Z","iopub.status.idle":"2024-04-27T14:23:00.835661Z","shell.execute_reply.started":"2024-04-27T14:23:00.827535Z","shell.execute_reply":"2024-04-27T14:23:00.834662Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \nimport os.path\nfrom os import path\n\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments,\\\n                        Trainer\nfrom datasets import Dataset\nimport pandas as pd\n\nog_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\ntrain1_df = og_train1[[\"tweet\",\"hard_label\"]].dropna().reset_index(drop=True)\ndev1_df = og_dev1[[\"tweet\",\"hard_label\"]].dropna().reset_index(drop=True)\n\nall_task_hard_labels = pd.concat([og_train1[\"hard_label\"],og_dev1[\"hard_label\"]]).dropna()\nall_task_hard_labels = all_task_hard_labels.apply(lambda x: eval(x)).tolist()\nprint(all_task_hard_labels[0])\n\ntrain1_df = task_hard_encode(train1_df)\n\ntrain1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\ntrain1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n\ndev1_df = task_hard_encode(dev1_df)\ndev1_df[\"tweet\"] = simple_preprocess(dev1_df[\"tweet\"])\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\nprint(\"train1\",train1_df.shape)\nprint(train1_df.head)\nprint(\"test1\",test1_df.shape)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:24:05.413673Z","iopub.execute_input":"2024-04-27T14:24:05.414362Z","iopub.status.idle":"2024-04-27T14:24:05.710892Z","shell.execute_reply.started":"2024-04-27T14:24:05.414336Z","shell.execute_reply":"2024-04-27T14:24:05.709798Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['OBJECTIFICATION', 'SEXUAL-VIOLENCE']\ntrain1 (6050, 2)\n<bound method NDFrame.head of                                                   tweet          hard_label\n0     @USER Ignora al otro, es un capullo.El problem...  [0, 0, 0, 1, 1, 0]\n1     @USER Si comicsgate se parece en algo a gamerg...  [0, 0, 1, 0, 0, 0]\n2     @USER Lee sobre Gamergate, y como eso ha cambi...  [0, 0, 1, 0, 0, 0]\n3     @USER @USER @USER Entonces como así es el merc...  [1, 0, 0, 1, 0, 1]\n4     @USER Aaah sí. Andrew Dobson. El que se dedicó...  [0, 0, 1, 0, 0, 0]\n...                                                 ...                 ...\n6045  idk why y’all bitches think having half your a...  [0, 1, 0, 1, 1, 1]\n6046  This has been a part of an experiment with @US...  [0, 0, 0, 1, 0, 0]\n6047  \"Take me already\" \"Not yet. You gotta be ready...  [0, 0, 0, 0, 1, 0]\n6048    @USER why do you look like a whore? /lh HTTPURL  [0, 1, 0, 1, 1, 1]\n6049  ik when mandy says “you look like a whore” i l...  [0, 0, 0, 1, 0, 1]\n\n[6050 rows x 2 columns]>\ntest1 (2076, 1)\n<bound method NDFrame.head of                                                   tweet\n0     @USER Todo gamergate desde el desarrollo hasta...\n1     @USER @USER Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @USER Primero fue internet, luego el gamergate...\n4     @USER Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @USER This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @USER @USER You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"train1_df.head","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:24:24.292367Z","iopub.execute_input":"2024-04-27T14:24:24.292786Z","iopub.status.idle":"2024-04-27T14:24:24.308351Z","shell.execute_reply.started":"2024-04-27T14:24:24.292739Z","shell.execute_reply":"2024-04-27T14:24:24.307276Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of                                                   tweet          hard_label\n0     @USER Ignora al otro, es un capullo.El problem...  [0, 0, 0, 1, 1, 0]\n1     @USER Si comicsgate se parece en algo a gamerg...  [0, 0, 1, 0, 0, 0]\n2     @USER Lee sobre Gamergate, y como eso ha cambi...  [0, 0, 1, 0, 0, 0]\n3     @USER @USER @USER Entonces como así es el merc...  [1, 0, 0, 1, 0, 1]\n4     @USER Aaah sí. Andrew Dobson. El que se dedicó...  [0, 0, 1, 0, 0, 0]\n...                                                 ...                 ...\n6045  idk why y’all bitches think having half your a...  [0, 1, 0, 1, 1, 1]\n6046  This has been a part of an experiment with @US...  [0, 0, 0, 1, 0, 0]\n6047  \"Take me already\" \"Not yet. You gotta be ready...  [0, 0, 0, 0, 1, 0]\n6048    @USER why do you look like a whore? /lh HTTPURL  [0, 1, 0, 1, 1, 1]\n6049  ik when mandy says “you look like a whore” i l...  [0, 0, 0, 1, 0, 1]\n\n[6050 rows x 2 columns]>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:24:50.788180Z","iopub.execute_input":"2024-04-27T14:24:50.788541Z","iopub.status.idle":"2024-04-27T14:24:50.814051Z","shell.execute_reply.started":"2024-04-27T14:24:50.788514Z","shell.execute_reply":"2024-04-27T14:24:50.813073Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer and model\nmodel_name = \"sdadas/xlm-roberta-large-twitter\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6,ignore_mismatched_sizes=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:24:52.872527Z","iopub.execute_input":"2024-04-27T14:24:52.873501Z","iopub.status.idle":"2024-04-27T14:25:10.728847Z","shell.execute_reply.started":"2024-04-27T14:24:52.873461Z","shell.execute_reply":"2024-04-27T14:25:10.727742Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/469 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ce2a58e34f4ed8956afefacae897a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ee16732e754eb384fa4eae37c12ce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b34a46d6f2543ed92c5f20d15bebcca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa4f48031844fb2a1ff01b9493e8ba9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdb82ffb048644d9b67dc8dfc1e1a677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d237e4e50e248a48ad3024d5b92b317"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 128\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    # accelerator = Accelerator(),\n    output_dir=\"./output/best-model/taks3/hard\",\n    num_train_epochs=4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate= 1.4822612607719694e-06, \n    weight_decay= 0.004509822968576173, \n    logging_dir=\"./logs\",\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:25:17.077920Z","iopub.execute_input":"2024-04-27T14:25:17.078285Z","iopub.status.idle":"2024-04-27T14:25:17.191870Z","shell.execute_reply.started":"2024-04-27T14:25:17.078257Z","shell.execute_reply":"2024-04-27T14:25:17.191061Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:25:19.429966Z","iopub.execute_input":"2024-04-27T14:25:19.430560Z","iopub.status.idle":"2024-04-27T14:25:19.435947Z","shell.execute_reply.started":"2024-04-27T14:25:19.430533Z","shell.execute_reply":"2024-04-27T14:25:19.435030Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:25:22.749151Z","iopub.execute_input":"2024-04-27T14:25:22.749984Z","iopub.status.idle":"2024-04-27T14:25:24.271347Z","shell.execute_reply.started":"2024-04-27T14:25:22.749952Z","shell.execute_reply":"2024-04-27T14:25:24.270579Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define the trainer for each fold\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:25:29.502490Z","iopub.execute_input":"2024-04-27T14:25:29.502919Z","iopub.status.idle":"2024-04-27T14:54:40.977210Z","shell.execute_reply.started":"2024-04-27T14:25:29.502890Z","shell.execute_reply":"2024-04-27T14:54:40.976222Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1484' max='1484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1484/1484 29:03, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.495900</td>\n      <td>0.395264</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.391900</td>\n      <td>0.351362</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1484, training_loss=0.39393833191247, metrics={'train_runtime': 1749.1749, 'train_samples_per_second': 13.568, 'train_steps_per_second': 0.848, 'total_flos': 5529223505946624.0, 'train_loss': 0.39393833191247, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Get predictions on the validation set\nfrom scipy.special import expit\nval_predictions = trainer.predict(val_dataset)\nval_pred_probabilities = expit(val_predictions.predictions)\n\nval_pred_labels = np.where(val_pred_probabilities > 0.5, 1, 0)\nval_true_labels = val_dataset[\"label\"]\n\n# Calculate evaluation metrics \nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels,average = \"weighted\")\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:54:52.060285Z","iopub.execute_input":"2024-04-27T14:54:52.060920Z","iopub.status.idle":"2024-04-27T14:55:09.876113Z","shell.execute_reply.started":"2024-04-27T14:54:52.060887Z","shell.execute_reply":"2024-04-27T14:55:09.874997Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.5038167938931297\nValidation F1 Score: 0.544934104994517\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions on the test set\nfrom scipy.special import expit\ntest_predictions = trainer.predict(test_dataset)\ntest_pred_probabilities = expit(test_predictions.predictions) #same as sigmoid\ntest_pred_labels = np.where(test_pred_probabilities > 0.5, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:55:16.271088Z","iopub.execute_input":"2024-04-27T14:55:16.271722Z","iopub.status.idle":"2024-04-27T14:55:52.176299Z","shell.execute_reply.started":"2024-04-27T14:55:16.271668Z","shell.execute_reply":"2024-04-27T14:55:52.175383Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"def create_results(probabilities, class_labels):\n    results = []\n    for prob_list in probabilities:\n        result_dict = {label: prob for label, prob in zip(class_labels, prob_list)}\n        results.append(result_dict)\n    return results\n\ntest_probs_list = create_results(test_pred_probabilities.tolist(), task_encoder.classes_)\ntest_probs_list[:10]","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:55:56.026871Z","iopub.execute_input":"2024-04-27T14:55:56.027255Z","iopub.status.idle":"2024-04-27T14:55:56.044677Z","shell.execute_reply.started":"2024-04-27T14:55:56.027226Z","shell.execute_reply":"2024-04-27T14:55:56.043728Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[{'IDEOLOGICAL-INEQUALITY': 0.0417814701795578,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.05180047079920769,\n  'NO': 0.9456539750099182,\n  'OBJECTIFICATION': 0.057388219982385635,\n  'SEXUAL-VIOLENCE': 0.05672871693968773,\n  'STEREOTYPING-DOMINANCE': 0.04743848741054535},\n {'IDEOLOGICAL-INEQUALITY': 0.04704846069216728,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.0583072230219841,\n  'NO': 0.9192383289337158,\n  'OBJECTIFICATION': 0.06456159055233002,\n  'SEXUAL-VIOLENCE': 0.0523686558008194,\n  'STEREOTYPING-DOMINANCE': 0.052323393523693085},\n {'IDEOLOGICAL-INEQUALITY': 0.05376865714788437,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.05004945769906044,\n  'NO': 0.9238836169242859,\n  'OBJECTIFICATION': 0.07103224843740463,\n  'SEXUAL-VIOLENCE': 0.05304911360144615,\n  'STEREOTYPING-DOMINANCE': 0.04925505071878433},\n {'IDEOLOGICAL-INEQUALITY': 0.5781364440917969,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.20313607156276703,\n  'NO': 0.13490647077560425,\n  'OBJECTIFICATION': 0.18775604665279388,\n  'SEXUAL-VIOLENCE': 0.12367677688598633,\n  'STEREOTYPING-DOMINANCE': 0.46229270100593567},\n {'IDEOLOGICAL-INEQUALITY': 0.03967639431357384,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.05483606085181236,\n  'NO': 0.9306992292404175,\n  'OBJECTIFICATION': 0.0655096098780632,\n  'SEXUAL-VIOLENCE': 0.052382443100214005,\n  'STEREOTYPING-DOMINANCE': 0.04599720984697342},\n {'IDEOLOGICAL-INEQUALITY': 0.03881030157208443,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.05545425042510033,\n  'NO': 0.9215840101242065,\n  'OBJECTIFICATION': 0.061270058155059814,\n  'SEXUAL-VIOLENCE': 0.06747660040855408,\n  'STEREOTYPING-DOMINANCE': 0.058074455708265305},\n {'IDEOLOGICAL-INEQUALITY': 0.35526344180107117,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.27449482679367065,\n  'NO': 0.09912589937448502,\n  'OBJECTIFICATION': 0.40528422594070435,\n  'SEXUAL-VIOLENCE': 0.30026036500930786,\n  'STEREOTYPING-DOMINANCE': 0.4429212510585785},\n {'IDEOLOGICAL-INEQUALITY': 0.07906708866357803,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.07589808851480484,\n  'NO': 0.7708966732025146,\n  'OBJECTIFICATION': 0.05236215889453888,\n  'SEXUAL-VIOLENCE': 0.04922978952527046,\n  'STEREOTYPING-DOMINANCE': 0.1035800650715828},\n {'IDEOLOGICAL-INEQUALITY': 0.05240180715918541,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.049375034868717194,\n  'NO': 0.8766211867332458,\n  'OBJECTIFICATION': 0.04823923856019974,\n  'SEXUAL-VIOLENCE': 0.067046619951725,\n  'STEREOTYPING-DOMINANCE': 0.0491923987865448},\n {'IDEOLOGICAL-INEQUALITY': 0.058209918439388275,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.05025480315089226,\n  'NO': 0.844951868057251,\n  'OBJECTIFICATION': 0.06519002467393875,\n  'SEXUAL-VIOLENCE': 0.04219695180654526,\n  'STEREOTYPING-DOMINANCE': 0.07046866416931152}]"},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id\"].astype(int)\n    output_dict = {}\n    decoded_labels = task_hard_decode(df)[\"hard_labels\"].tolist()\n    print(\"decoded labels length:\", len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = list(decoded_labels[i])\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"./output/task3_NICA-1.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(pd.Series(test_pred_labels.tolist()), columns=[\"hard_label\"]), test_probs_list, og_test1[[\"id\"]],og_test1)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:56:10.709560Z","iopub.execute_input":"2024-04-27T14:56:10.710287Z","iopub.status.idle":"2024-04-27T14:56:10.834736Z","shell.execute_reply.started":"2024-04-27T14:56:10.710248Z","shell.execute_reply":"2024-04-27T14:56:10.833859Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"decoded labels length: 2076\noutput generated as json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Soft","metadata":{}},{"cell_type":"code","source":"#relevant functions\nimport re\n# Create function for printing \ndef print_custom(text):\n    print('\\n')\n    print(text)\n    print('-'*100)\n    \ndef simple_preprocess(text):\n    \"\"\"\n    pass the tweet data as a series. do not use apply function\n    only preprocesses for replacing @USER and URLS\n    \"\"\"\n    # print(\"i am preprocessing\")\n    URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n    HANDLE_RE = re.compile(r\"@\\w+\")\n    tweets = []\n    for t in text:\n        t = HANDLE_RE.sub(\"@USER\", t)\n        t = URL_RE.sub(\"HTTPURL\", t)\n        tweets.append(t)\n    return tweets\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndef convert_logits_to_list(logits_dict):\n    \"\"\"\n    order:\n    1. classes are \n    OBJECTIFICATION,'SEXUAL-VIOLENCE',NO,STEREOTYPING-DOMINANCE,IDEOLOGICAL-INEQUALITY,MISOGYNY-NON-SEXUAL-VIOLENCE\n    \"\"\"\n    logits_dict = eval(logits_dict)\n    logits_list = [logits_dict[\"OBJECTIFICATION\"], logits_dict[\"SEXUAL-VIOLENCE\"],logits_dict[\"NO\"],logits_dict[\"STEREOTYPING-DOMINANCE\"],logits_dict[\"IDEOLOGICAL-INEQUALITY\"], logits_dict[\"MISOGYNY-NON-SEXUAL-VIOLENCE\"]]\n    return logits_list\n\ndef convert_list_to_logits(logits_list):\n    logits_dict = {\"OBJECTIFICATION\": logits_list[0], \"SEXUAL-VIOLENCE\": logits_list[1],\"NO\": logits_list[2], \"STEREOTYPING-DOMINANCE\": logits_list[3],\"IDEOLOGICAL-INEQUALITY\": logits_list[4], \"MISOGYNY-NON-SEXUAL-VIOLENCE\": logits_list[5]}\n    return logits_dict\n\ndef check_dtype(given_data):\n    return eval(given_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:57:59.821003Z","iopub.execute_input":"2024-04-27T14:57:59.821860Z","iopub.status.idle":"2024-04-27T14:57:59.831975Z","shell.execute_reply.started":"2024-04-27T14:57:59.821826Z","shell.execute_reply":"2024-04-27T14:57:59.831082Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \nimport os.path\nfrom os import path\n\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\nfrom datasets import Dataset\n\n\n#load given data\nimport pandas as pd\nog_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\nog_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\nog_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\ntrain1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n\n\nog_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\nog_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\ndev1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\n\nprint(train1_df.head)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:58:23.089177Z","iopub.execute_input":"2024-04-27T14:58:23.089569Z","iopub.status.idle":"2024-04-27T14:58:23.379677Z","shell.execute_reply.started":"2024-04-27T14:58:23.089539Z","shell.execute_reply":"2024-04-27T14:58:23.378740Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"<bound method NDFrame.head of                                                   tweet  \\\n0     @USER Ignora al otro, es un capullo.El problem...   \n1     @USER Si comicsgate se parece en algo a gamerg...   \n2     @USER Lee sobre Gamergate, y como eso ha cambi...   \n3     @USER Un retraso social bastante lamentable, g...   \n4     @USER @USER @USER Entonces como así es el merc...   \n...                                                 ...   \n6915  idk why y’all bitches think having half your a...   \n6916  This has been a part of an experiment with @US...   \n6917  \"Take me already\" \"Not yet. You gotta be ready...   \n6918    @USER why do you look like a whore? /lh HTTPURL   \n6919  ik when mandy says “you look like a whore” i l...   \n\n                                             soft_label  \n0     [0.33333333333333304, 0.33333333333333304, 0.1...  \n1     [0.16666666666666602, 0.0, 0.833333333333333, ...  \n2                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n3     [0.16666666666666602, 0.16666666666666602, 0.5...  \n4     [0.5, 0.0, 0.33333333333333304, 0.5, 0.3333333...  \n...                                                 ...  \n6915  [0.6666666666666661, 0.6666666666666661, 0.0, ...  \n6916  [0.6666666666666661, 0.0, 0.0, 0.1666666666666...  \n6917  [0.16666666666666602, 0.33333333333333304, 0.3...  \n6918  [0.6666666666666661, 0.5, 0.0, 0.3333333333333...  \n6919  [0.5, 0.16666666666666602, 0.16666666666666602...  \n\n[6920 rows x 2 columns]>\n<bound method NDFrame.head of                                                   tweet\n0     @USER Todo gamergate desde el desarrollo hasta...\n1     @USER @USER Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @USER Primero fue internet, luego el gamergate...\n4     @USER Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @USER This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @USER @USER You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"train1_df.info","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:58:40.839198Z","iopub.execute_input":"2024-04-27T14:58:40.839940Z","iopub.status.idle":"2024-04-27T14:58:40.850569Z","shell.execute_reply.started":"2024-04-27T14:58:40.839909Z","shell.execute_reply":"2024-04-27T14:58:40.849613Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<bound method DataFrame.info of                                                   tweet  \\\n0     @USER Ignora al otro, es un capullo.El problem...   \n1     @USER Si comicsgate se parece en algo a gamerg...   \n2     @USER Lee sobre Gamergate, y como eso ha cambi...   \n3     @USER Un retraso social bastante lamentable, g...   \n4     @USER @USER @USER Entonces como así es el merc...   \n...                                                 ...   \n6915  idk why y’all bitches think having half your a...   \n6916  This has been a part of an experiment with @US...   \n6917  \"Take me already\" \"Not yet. You gotta be ready...   \n6918    @USER why do you look like a whore? /lh HTTPURL   \n6919  ik when mandy says “you look like a whore” i l...   \n\n                                             soft_label  \n0     [0.33333333333333304, 0.33333333333333304, 0.1...  \n1     [0.16666666666666602, 0.0, 0.833333333333333, ...  \n2                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n3     [0.16666666666666602, 0.16666666666666602, 0.5...  \n4     [0.5, 0.0, 0.33333333333333304, 0.5, 0.3333333...  \n...                                                 ...  \n6915  [0.6666666666666661, 0.6666666666666661, 0.0, ...  \n6916  [0.6666666666666661, 0.0, 0.0, 0.1666666666666...  \n6917  [0.16666666666666602, 0.33333333333333304, 0.3...  \n6918  [0.6666666666666661, 0.5, 0.0, 0.3333333333333...  \n6919  [0.5, 0.16666666666666602, 0.16666666666666602...  \n\n[6920 rows x 2 columns]>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:59:05.988687Z","iopub.execute_input":"2024-04-27T14:59:05.989378Z","iopub.status.idle":"2024-04-27T14:59:06.002066Z","shell.execute_reply.started":"2024-04-27T14:59:05.989345Z","shell.execute_reply":"2024-04-27T14:59:06.001213Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer and model\nmodel_name = \"sdadas/xlm-roberta-large-twitter\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6,ignore_mismatched_sizes=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 128\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/task3/soft\",\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate= 8.036768439133452e-06, \n    weight_decay= 8.398333136282262e-05, \n    logging_dir=\"./logs\",\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:11:08.318273Z","iopub.execute_input":"2024-04-27T15:11:08.318642Z","iopub.status.idle":"2024-04-27T15:11:08.352021Z","shell.execute_reply.started":"2024-04-27T15:11:08.318614Z","shell.execute_reply":"2024-04-27T15:11:08.351241Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\ndef convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\n# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:11:10.621235Z","iopub.execute_input":"2024-04-27T15:11:10.621585Z","iopub.status.idle":"2024-04-27T15:11:12.673820Z","shell.execute_reply.started":"2024-04-27T15:11:10.621557Z","shell.execute_reply":"2024-04-27T15:11:12.672837Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#with cross entropy loss and label smoothing\ndef custom_loss_fn(logits, soft_labels):\n    probs = torch.sigmoid(logits)\n    # Apply nn.CrossEntropyLoss\n    loss = nn.CrossEntropyLoss(reduction=\"sum\",label_smoothing=0.15)(probs, soft_labels)\n    return loss\n\n# Define the trainer for each fold\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = custom_loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:11:25.181231Z","iopub.execute_input":"2024-04-27T15:11:25.181996Z","iopub.status.idle":"2024-04-27T15:11:25.188595Z","shell.execute_reply.started":"2024-04-27T15:11:25.181962Z","shell.execute_reply":"2024-04-27T15:11:25.187509Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:11:27.270089Z","iopub.execute_input":"2024-04-27T15:11:27.270881Z","iopub.status.idle":"2024-04-27T15:11:29.907266Z","shell.execute_reply.started":"2024-04-27T15:11:27.270849Z","shell.execute_reply":"2024-04-27T15:11:29.905980Z"},"trusted":true},"execution_count":33,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model for each fold\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2181\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 2181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2182\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:599\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    597\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 599\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    602\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 17.06 MiB is free. Process 3865 has 14.73 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 96.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 17.06 MiB is free. Process 3865 has 14.73 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 96.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom scipy.special import expit\n\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_probabilities = expit(val_predictions.predictions)\n\nval_pred_labels = np.where(val_pred_probabilities > 0.5, 1, 0)\nval_true_labels = [[1 if value > 0.5 else 0 for value in sublist] for sublist in val_dataset[\"label\"]]\n\n# Calculate evaluation metrics \nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels,average = \"weighted\")\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions on the test set\ntest_predictions1 = trainer.predict(test_dataset)\n\ntest_pred_probabilities1 = expit(test_predictions1.predictions) #, axis=1 #same as sigmoid\ntest_pred_labels1 = np.where(test_pred_probabilities1 > 0.5, 1, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_results(probabilities, class_labels):\n    results = []\n    for prob_list in probabilities:\n        result_dict = {label: prob for label, prob in zip(class_labels, prob_list)}\n        results.append(result_dict)\n    return results\n\nclasses_labels = [\"OBJECTIFICATION\",\"SEXUAL-VIOLENCE\",\"NO\",\"STEREOTYPING-DOMINANCE\",\"IDEOLOGICAL-INEQUALITY\",\"MISOGYNY-NON-SEXUAL-VIOLENCE\"]\ntest_probs_list1 = create_results(test_pred_probabilities1.tolist(), classes_labels)\ntest_probs_list1[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id_EXIST\"].astype(int)\n    output_dict = {}\n    decoded_labels = task_hard_decode(df)[\"hard_labels\"].tolist()\n    print(\"decoded labels length:\",len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = list(decoded_labels[i])\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"./output/task3_NICA-2.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(pd.Series(test_pred_labels1.tolist()),columns=[\"hard_label\"]), test_probs_list, og_test1[[\"id\"]],og_test1)","metadata":{},"execution_count":null,"outputs":[]}]}