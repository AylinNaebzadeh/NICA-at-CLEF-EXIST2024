{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8242355,"sourceType":"datasetVersion","datasetId":4889218}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-02T07:13:30.989703Z","iopub.execute_input":"2024-05-02T07:13:30.990052Z","iopub.status.idle":"2024-05-02T07:13:31.363621Z","shell.execute_reply.started":"2024-05-02T07:13:30.990022Z","shell.execute_reply":"2024-05-02T07:13:31.362702Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/train_data_task1.csv\n/kaggle/input/dev_data_task3.csv\n/kaggle/input/train_data_task2.csv\n/kaggle/input/dev_data_task1.csv\n/kaggle/input/train_data_task3.csv\n/kaggle/input/EXIT2024_tweet_test.csv\n/kaggle/input/dev_data_task2.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/output/best-model/*","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:41:30.338138Z","iopub.execute_input":"2024-05-01T06:41:30.339405Z","iopub.status.idle":"2024-05-01T06:41:31.976927Z","shell.execute_reply.started":"2024-05-01T06:41:30.339358Z","shell.execute_reply":"2024-05-01T06:41:31.975686Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb offline\n!wandb disabled","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:13:37.681069Z","iopub.execute_input":"2024-05-02T07:13:37.682061Z","iopub.status.idle":"2024-05-02T07:13:41.636944Z","shell.execute_reply.started":"2024-05-02T07:13:37.682028Z","shell.execute_reply":"2024-05-02T07:13:41.635905Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\nW&B disabled.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:13:42.674959Z","iopub.execute_input":"2024-05-02T07:13:42.675356Z","iopub.status.idle":"2024-05-02T07:14:08.855637Z","shell.execute_reply.started":"2024-05-02T07:13:42.675321Z","shell.execute_reply":"2024-05-02T07:14:08.854458Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from  IPython. display import clear_output\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport pandas as pd\nimport numpy as np\nimport random\nimport  matplotlib. pyplot  as  plt\nfrom tqdm import tqdm\n\nimport torch\nimport torch.optim as optim\nimport torch. nn.functional as F\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:14:27.255287Z","iopub.execute_input":"2024-05-02T07:14:27.255939Z","iopub.status.idle":"2024-05-02T07:14:43.385830Z","shell.execute_reply.started":"2024-05-02T07:14:27.255905Z","shell.execute_reply":"2024-05-02T07:14:43.384878Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-02 07:14:35.214431: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-02 07:14:35.214524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-02 07:14:35.313782: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/train_data_task1.csv')\ntest_df = pd.read_csv('/kaggle/input/EXIT2024_tweet_test.csv')\ndev_df = pd.read_csv('/kaggle/input/dev_data_task1.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:14:43.387714Z","iopub.execute_input":"2024-05-02T07:14:43.388296Z","iopub.status.idle":"2024-05-02T07:14:43.495005Z","shell.execute_reply.started":"2024-05-02T07:14:43.388269Z","shell.execute_reply":"2024-05-02T07:14:43.493915Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T05:47:43.504610Z","iopub.execute_input":"2024-05-01T05:47:43.504980Z","iopub.status.idle":"2024-05-01T05:47:43.546108Z","shell.execute_reply.started":"2024-05-01T05:47:43.504949Z","shell.execute_reply":"2024-05-01T05:47:43.545025Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2076 entries, 0 to 2075\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      2076 non-null   int64 \n 1   lang    2076 non-null   object\n 2   tweet   2076 non-null   object\n 3   split   2076 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 65.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T11:41:46.830848Z","iopub.execute_input":"2024-04-23T11:41:46.831499Z","iopub.status.idle":"2024-04-23T11:41:46.847794Z","shell.execute_reply.started":"2024-04-23T11:41:46.831469Z","shell.execute_reply":"2024-04-23T11:41:46.846849Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"       id lang                                              tweet    split\n0  500001   es  @Eurogamer_es Todo gamergate desde el desarrol...  TEST_ES\n1  500002   es  @ArCaNgEl__23 @Benzenazi Hombre, no es compara...  TEST_ES\n2  500003   es  yo buscando las empresas metidas en el gamerga...  TEST_ES\n3  500004   es  @jordirico Primero fue internet, luego el game...  TEST_ES\n4  500005   es  @AlonsoQuijano12 Yo estuve metido en el gamerg...  TEST_ES","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>lang</th>\n      <th>tweet</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>500001</td>\n      <td>es</td>\n      <td>@Eurogamer_es Todo gamergate desde el desarrol...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>500002</td>\n      <td>es</td>\n      <td>@ArCaNgEl__23 @Benzenazi Hombre, no es compara...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500003</td>\n      <td>es</td>\n      <td>yo buscando las empresas metidas en el gamerga...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>500004</td>\n      <td>es</td>\n      <td>@jordirico Primero fue internet, luego el game...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500005</td>\n      <td>es</td>\n      <td>@AlonsoQuijano12 Yo estuve metido en el gamerg...</td>\n      <td>TEST_ES</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def simple_preprocess(text):\n    \"\"\"\n    pass the tweet data as a series. do not use apply function\n    only preprocesses for replacing @USER and URLS\n    \"\"\"\n    URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n    HANDLE_RE = re.compile(r\"@\\w+\")\n    tweets = []\n    for t in text:\n        t = HANDLE_RE.sub(\"@user\", t)\n        t = URL_RE.sub(\"http\", t)\n        tweets.append(t)\n    return tweets","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:14:43.497075Z","iopub.execute_input":"2024-05-02T07:14:43.497493Z","iopub.status.idle":"2024-05-02T07:14:43.575787Z","shell.execute_reply.started":"2024-05-02T07:14:43.497465Z","shell.execute_reply":"2024-05-02T07:14:43.574613Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport os.path\nfrom os import path\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\nfrom datasets import Dataset\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:14:43.578838Z","iopub.execute_input":"2024-05-02T07:14:43.579778Z","iopub.status.idle":"2024-05-02T07:14:43.608694Z","shell.execute_reply.started":"2024-05-02T07:14:43.579739Z","shell.execute_reply":"2024-05-02T07:14:43.607935Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Hard","metadata":{}},{"cell_type":"code","source":"#instantiate label encoders\ntask1_encoder = LabelEncoder()\n\ndef task1_hard_encode(df):\n    task1_encoder.fit(all_task1_hard_labels)\n    df['hard_label'] = task1_encoder.transform(df['hard_label'])\n    return df\n\ndef task1_hard_decode(df):\n    df[\"hard_label\"] = task1_encoder.inverse_transform(df[\"hard_label\"])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:14:43.610093Z","iopub.execute_input":"2024-05-02T07:14:43.610422Z","iopub.status.idle":"2024-05-02T07:14:43.615971Z","shell.execute_reply.started":"2024-05-02T07:14:43.610393Z","shell.execute_reply":"2024-05-02T07:14:43.614945Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"og_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\nall_task1_hard_labels = pd.concat([og_train1[\"hard_label\"], og_dev1[\"hard_label\"]])\ntrain1_df = task1_hard_encode(og_train1)\n\n# print(train1_df.columns)\ntrain1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\ntrain1_df = train1_df[train1_df['hard_label'] != 2]\ntrain1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n\ndev1_df = task1_hard_encode(og_dev1)\ndev1_df = dev1_df[[\"tweet\",\"hard_label\"]].dropna()\ndev1_df = dev1_df[dev1_df['hard_label'] != 2]\ndev1_df[\"tweet\"] = simple_preprocess(dev1_df[\"tweet\"])\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\n# test1_df = test1_df[test1_df['hard_label'] != 2]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\nprint(\"train1\",train1_df.shape)\nprint(train1_df.head)\nprint(\"test1\",test1_df.shape)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:14:43.617262Z","iopub.execute_input":"2024-05-02T07:14:43.617608Z","iopub.status.idle":"2024-05-02T07:14:43.683690Z","shell.execute_reply.started":"2024-05-02T07:14:43.617578Z","shell.execute_reply":"2024-05-02T07:14:43.682702Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"train1 (6064, 2)\n<bound method NDFrame.head of                                                   tweet  hard_label\n0     @user Ignora al otro, es un capullo.El problem...           1\n1     @user Si comicsgate se parece en algo a gamerg...           0\n2     @user Lee sobre Gamergate, y como eso ha cambi...           0\n4     @user @user @user Entonces como así es el merc...           1\n5     @user Aaah sí. Andrew Dobson. El que se dedicó...           0\n...                                                 ...         ...\n6915  idk why y’all bitches think having half your a...           1\n6916  This has been a part of an experiment with @us...           1\n6917  \"Take me already\" \"Not yet. You gotta be ready...           1\n6918       @user why do you look like a whore? /lh http           1\n6919  ik when mandy says “you look like a whore” i l...           1\n\n[6064 rows x 2 columns]>\ntest1 (2076, 1)\n<bound method NDFrame.head of                                                   tweet\n0     @user Todo gamergate desde el desarrollo hasta...\n1     @user @user Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @user Primero fue internet, luego el gamergate...\n4     @user Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @user This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @user @user You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:14:43.685029Z","iopub.execute_input":"2024-05-02T07:14:43.685330Z","iopub.status.idle":"2024-05-02T07:14:43.697981Z","shell.execute_reply.started":"2024-05-02T07:14:43.685304Z","shell.execute_reply":"2024-05-02T07:14:43.696831Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# with GPT2","metadata":{}},{"cell_type":"code","source":"device  =  torch. device('cuda:0'  if  torch. cuda. is_available() else  'cpu')\nprint(f\"computation will run on {device} now\")","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:14:43.699050Z","iopub.execute_input":"2024-05-02T07:14:43.699409Z","iopub.status.idle":"2024-05-02T07:14:43.759730Z","shell.execute_reply.started":"2024-05-02T07:14:43.699384Z","shell.execute_reply":"2024-05-02T07:14:43.758603Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"computation will run on cuda:0 now\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (set_seed,\n                          TrainingArguments,\n                          Trainer,\n                          GPT2Config,\n                          GPT2Tokenizer,\n                          AdamW, \n                          get_linear_schedule_with_warmup,\n                          GPT2ForSequenceClassification)\n# tokenizer = AutoTokenizer.from_pretrained('openai-community/openai-gpt')\n# model = AutoModelForSequenceClassification.from_pretrained(\"openai-community/openai-gpt\", num_labels=2,ignore_mismatched_sizes=True)\n\nprint('Loading gpt-2 model')\nmodel_config = GPT2Config.from_pretrained(pretrained_model_name_or_path='gpt2', num_labels=2)\n\nprint('Loading tokenizer...')\ntokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path='gpt2')\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.eos_token\n\nprint('Loading model...')\nmodel = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path='gpt2', config=model_config)\nmodel.resize_token_embeddings(len(tokenizer)) \nmodel.config.pad_token_id = model.config.eos_token_id\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:15:03.726618Z","iopub.execute_input":"2024-05-02T07:15:03.727247Z","iopub.status.idle":"2024-05-02T07:15:08.403478Z","shell.execute_reply.started":"2024-05-02T07:15:03.727218Z","shell.execute_reply":"2024-05-02T07:15:08.402579Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Loading gpt-2 model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1c096db7f754564821a78687030b6d5"}},"metadata":{}},{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e871c6abc3934356b49f9de9f36d0b5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d2afc87e3a4185a1bdbf8f841145f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17ec28aa87f44730862fe2fcacdbd2a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f3d0a14a35d41efa1390f6499e2a032"}},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0a9b56b49e4476ab76a27bce0270dc"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"GPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=2, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import (set_seed,\n                          TrainingArguments,\n                          Trainer,\n                          GPT2Config,\n                          GPT2Tokenizer,\n                          AdamW, \n                          get_linear_schedule_with_warmup,\n                          GPT2ForSequenceClassification)\n# tokenizer = AutoTokenizer.from_pretrained('openai-community/openai-gpt')\n# model = AutoModelForSequenceClassification.from_pretrained(\"openai-community/openai-gpt\", num_labels=2,ignore_mismatched_sizes=True)\n\nprint('Loading tokenizer...')\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='ai-forever/mGPT')\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.eos_token\n\nprint('Loading model...')\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path='ai-forever/mGPT', num_labels=2)\nmodel.resize_token_embeddings(len(tokenizer)) \nmodel.config.pad_token_id = model.config.eos_token_id\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:30:49.511013Z","iopub.execute_input":"2024-05-02T07:30:49.511790Z","iopub.status.idle":"2024-05-02T07:31:07.786359Z","shell.execute_reply.started":"2024-05-02T07:30:49.511758Z","shell.execute_reply":"2024-05-02T07:31:07.785502Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Loading tokenizer...\nLoading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95dbb3c45df041929cfac1badcd48246"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ai-forever/mGPT and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"GPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(100000, 2048)\n    (wpe): Embedding(2048, 2048)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=2048, out_features=2, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:15:13.354056Z","iopub.execute_input":"2024-05-02T07:15:13.354920Z","iopub.status.idle":"2024-05-02T07:15:13.359267Z","shell.execute_reply.started":"2024-05-02T07:15:13.354886Z","shell.execute_reply":"2024-05-02T07:15:13.358156Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Training arguments for GPT large","metadata":{}},{"cell_type":"markdown","source":"https://datascience.stackexchange.com/questions/64583/what-are-the-good-parameter-ranges-for-bert-hyperparameters-while-finetuning-it","metadata":{}},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 256\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/best-model-mgpt\",\n    report_to=None,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=1e-5,\n    weight_decay=0.01,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:31:31.676454Z","iopub.execute_input":"2024-05-02T07:31:31.677160Z","iopub.status.idle":"2024-05-02T07:31:31.710982Z","shell.execute_reply.started":"2024-05-02T07:31:31.677124Z","shell.execute_reply":"2024-05-02T07:31:31.710104Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 256\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/best-model\",\n    report_to=None,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:28:07.702587Z","iopub.execute_input":"2024-05-02T07:28:07.703284Z","iopub.status.idle":"2024-05-02T07:28:07.737044Z","shell.execute_reply.started":"2024-05-02T07:28:07.703253Z","shell.execute_reply":"2024-05-02T07:28:07.736267Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 128\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/best-model\",\n    report_to=None,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:42:38.779127Z","iopub.execute_input":"2024-05-01T06:42:38.779493Z","iopub.status.idle":"2024-05-01T06:42:38.816422Z","shell.execute_reply.started":"2024-05-01T06:42:38.779467Z","shell.execute_reply":"2024-05-01T06:42:38.815609Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 128\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/best-model\",\n    report_to=None,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=4e-5,\n    weight_decay=0.01,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:58:02.927953Z","iopub.execute_input":"2024-05-01T06:58:02.928842Z","iopub.status.idle":"2024-05-01T06:58:02.963040Z","shell.execute_reply.started":"2024-05-01T06:58:02.928804Z","shell.execute_reply":"2024-05-01T06:58:02.962240Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:15:28.681765Z","iopub.execute_input":"2024-05-02T07:15:28.682576Z","iopub.status.idle":"2024-05-02T07:15:28.689221Z","shell.execute_reply.started":"2024-05-02T07:15:28.682531Z","shell.execute_reply":"2024-05-02T07:15:28.688343Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"],return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], padding=True, truncation=True, max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], padding=True, truncation=True, max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:31:41.411454Z","iopub.execute_input":"2024-05-02T07:31:41.411915Z","iopub.status.idle":"2024-05-02T07:31:43.843517Z","shell.execute_reply.started":"2024-05-02T07:31:41.411878Z","shell.execute_reply":"2024-05-02T07:31:43.842706Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def compute_f1_score(pred):\n    # pred is a tuple (predictions, labels)\n    predictions, labels = pred\n    # Compute the F1 score\n    f1 = f1_score(labels, predictions.argmax(axis=1), average='macro')\n    return {\"f1_score\": f1}","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:15:41.439413Z","iopub.execute_input":"2024-05-02T07:15:41.439796Z","iopub.status.idle":"2024-05-02T07:15:41.444878Z","shell.execute_reply.started":"2024-05-02T07:15:41.439763Z","shell.execute_reply":"2024-05-02T07:15:41.443947Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"optimizers (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional, defaults to (None, None)) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your model and a scheduler given by get_linear_schedule_with_warmup() controlled by args.","metadata":{}},{"cell_type":"markdown","source":"# with lr=1e-5","metadata":{}},{"cell_type":"code","source":"# model.to(device)\n# Define the trainer for each fold\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_f1_score\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:15:44.962864Z","iopub.execute_input":"2024-05-02T07:15:44.963512Z","iopub.status.idle":"2024-05-02T07:26:31.771486Z","shell.execute_reply.started":"2024-05-02T07:15:44.963479Z","shell.execute_reply":"2024-05-02T07:26:31.770518Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1860' max='1860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1860/1860 10:40, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.701400</td>\n      <td>0.653078</td>\n      <td>0.626795</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.642900</td>\n      <td>0.556140</td>\n      <td>0.709708</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.539400</td>\n      <td>0.544216</td>\n      <td>0.728638</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1860, training_loss=0.5939103403399068, metrics={'train_runtime': 645.6891, 'train_samples_per_second': 46.059, 'train_steps_per_second': 2.881, 'total_flos': 1942741340651520.0, 'train_loss': 0.5939103403399068, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"# mGPT\n# Define the trainer for each fold\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_f1_score\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:32:03.809424Z","iopub.execute_input":"2024-05-02T07:32:03.809805Z","iopub.status.idle":"2024-05-02T07:32:07.064369Z","shell.execute_reply.started":"2024-05-02T07:32:03.809773Z","shell.execute_reply":"2024-05-02T07:32:07.063017Z"},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_f1_score\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model for each fold\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1426, in forward\n    transformer_outputs = self.transformer(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n    outputs = block(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 427, in forward\n    feed_forward_hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 354, in forward\n    hidden_states = self.c_fc(hidden_states)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pytorch_utils.py\", line 103, in forward\n    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 33.06 MiB is free. Process 2088 has 14.71 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 289.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"],"ename":"OutOfMemoryError","evalue":"Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1426, in forward\n    transformer_outputs = self.transformer(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 888, in forward\n    outputs = block(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 427, in forward\n    feed_forward_hidden_states = self.mlp(hidden_states)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 354, in forward\n    hidden_states = self.c_fc(hidden_states)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pytorch_utils.py\", line 103, in forward\n    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 33.06 MiB is free. Process 2088 has 14.71 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 289.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","output_type":"error"}]},{"cell_type":"markdown","source":"# with lr=3e-5","metadata":{}},{"cell_type":"code","source":"# model.to(device)\n# Define the trainer for each fold\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_f1_score\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:43:56.045170Z","iopub.execute_input":"2024-05-01T06:43:56.046122Z","iopub.status.idle":"2024-05-01T06:55:47.307197Z","shell.execute_reply.started":"2024-05-01T06:43:56.046068Z","shell.execute_reply":"2024-05-01T06:55:47.306053Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1860' max='1860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1860/1860 11:50, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.402800</td>\n      <td>0.275215</td>\n      <td>0.900871</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.227700</td>\n      <td>0.385791</td>\n      <td>0.904570</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.091400</td>\n      <td>0.404047</td>\n      <td>0.913214</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1860, training_loss=0.18115151825771536, metrics={'train_runtime': 710.327, 'train_samples_per_second': 41.868, 'train_steps_per_second': 2.619, 'total_flos': 1956230696601600.0, 'train_loss': 0.18115151825771536, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# with lr=4e-5","metadata":{}},{"cell_type":"code","source":"# model.to(device)\n# Define the trainer for each fold\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_f1_score\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:58:46.107037Z","iopub.execute_input":"2024-05-01T06:58:46.107432Z","iopub.status.idle":"2024-05-01T07:10:38.090043Z","shell.execute_reply.started":"2024-05-01T06:58:46.107399Z","shell.execute_reply":"2024-05-01T07:10:38.089069Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1860' max='1860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1860/1860 11:51, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.248800</td>\n      <td>0.545815</td>\n      <td>0.859971</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.206200</td>\n      <td>0.502150</td>\n      <td>0.877194</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.066700</td>\n      <td>0.529500</td>\n      <td>0.899541</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1860, training_loss=0.13659687965146958, metrics={'train_runtime': 711.1077, 'train_samples_per_second': 41.822, 'train_steps_per_second': 2.616, 'total_flos': 1956230696601600.0, 'train_loss': 0.13659687965146958, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# with lr=1e-5","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_labels = np.argmax(val_predictions.predictions, axis=1)\nval_true_labels = val_dataset[\"label\"]\n\nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels)\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:26:49.584314Z","iopub.execute_input":"2024-05-02T07:26:49.585328Z","iopub.status.idle":"2024-05-02T07:26:56.032581Z","shell.execute_reply.started":"2024-05-02T07:26:49.585284Z","shell.execute_reply":"2024-05-02T07:26:56.031518Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.7295238095238096\nValidation F1 Score: 0.7441441441441442\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# with lr=3e-05","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_labels = np.argmax(val_predictions.predictions, axis=1)\nval_true_labels = val_dataset[\"label\"]\n\nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels)\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:56:24.483360Z","iopub.execute_input":"2024-05-01T06:56:24.483734Z","iopub.status.idle":"2024-05-01T06:56:30.625028Z","shell.execute_reply.started":"2024-05-01T06:56:24.483704Z","shell.execute_reply":"2024-05-01T06:56:30.623904Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.900952380952381\nValidation F1 Score: 0.8980392156862745\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# with lr=4e-5","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_labels = np.argmax(val_predictions.predictions, axis=1)\nval_true_labels = val_dataset[\"label\"]\n\nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels)\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:11:13.839081Z","iopub.execute_input":"2024-05-01T07:11:13.839500Z","iopub.status.idle":"2024-05-01T07:11:19.983217Z","shell.execute_reply.started":"2024-05-01T07:11:13.839468Z","shell.execute_reply":"2024-05-01T07:11:19.982114Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.8790476190476191\nValidation F1 Score: 0.8621064060803475\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions on the test set\nfrom scipy.special import softmax\ntest_predictions = trainer.predict(test_dataset)\ntest_pred_labels = np.argmax(test_predictions.predictions, axis=1)\ntest_pred_probs = softmax(test_predictions.predictions).tolist()\n\n#get probabilities to a list\ntest_probs_list = [] \nfor logits in test_pred_probs:\n    test_probs_list.append({'YES': logits[0], 'NO': logits[1]})","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:27:02.950487Z","iopub.execute_input":"2024-05-02T07:27:02.951328Z","iopub.status.idle":"2024-05-02T07:27:15.702677Z","shell.execute_reply.started":"2024-05-02T07:27:02.951293Z","shell.execute_reply":"2024-05-02T07:27:15.701898Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id\"].astype(int)\n    output_dict = {}\n    decoded_labels = task1_hard_decode(df)[\"hard_label\"].tolist()\n    print(\"decoded labels length:\",len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = decoded_labels[i]\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"./output/task1_NICA-hard-8.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(test_pred_labels,columns=[\"hard_label\"]), test_probs_list, og_test1[[\"id\"]],og_test1)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T07:27:41.466253Z","iopub.execute_input":"2024-05-02T07:27:41.466914Z","iopub.status.idle":"2024-05-02T07:27:41.547004Z","shell.execute_reply.started":"2024-05-02T07:27:41.466880Z","shell.execute_reply":"2024-05-02T07:27:41.546214Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"decoded labels length: 2076\noutput generated as json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Soft","metadata":{}},{"cell_type":"code","source":"device  =  torch. device('cuda:0'  if  torch. cuda. is_available() else  'cpu')\nprint(f\"computation will run on {device} now\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:20:21.473106Z","iopub.execute_input":"2024-04-24T13:20:21.473549Z","iopub.status.idle":"2024-04-24T13:20:21.479104Z","shell.execute_reply.started":"2024-04-24T13:20:21.473518Z","shell.execute_reply":"2024-04-24T13:20:21.478213Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"computation will run on cuda:0 now\n","output_type":"stream"}]},{"cell_type":"code","source":"#instantiate label encoders\ndef convert_logits_to_list(logits_dict):\n    logits_dict = eval(logits_dict)\n    logits_list = [logits_dict[\"YES\"], logits_dict[\"NO\"]]\n    return logits_list\n\ndef convert_list_to_logits(logits_list):\n    logits_dict = {\"YES\": logits_list[0], \"NO\": logits_list[1]}\n    return logits_dict\n\ndef check_dtype(given_data):\n    return eval(given_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:17:51.020947Z","iopub.execute_input":"2024-05-01T06:17:51.021609Z","iopub.status.idle":"2024-05-01T06:17:51.027904Z","shell.execute_reply.started":"2024-05-01T06:17:51.021575Z","shell.execute_reply":"2024-05-01T06:17:51.026598Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/exist-2024-clef/train_data_task1.csv')\ntest_df = pd.read_csv('/kaggle/input/exist-2024-clef/EXIT2024_tweet_test.csv')\ndev_df = pd.read_csv('/kaggle/input/exist-2024-clef/dev_data_task1.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:43:24.980650Z","iopub.execute_input":"2024-05-01T07:43:24.981428Z","iopub.status.idle":"2024-05-01T07:43:25.054834Z","shell.execute_reply.started":"2024-05-01T07:43:24.981389Z","shell.execute_reply":"2024-05-01T07:43:25.053734Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"og_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\nog_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\nog_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\ntrain1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n\n\nog_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\nog_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\ndev1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\nprint(\"train1\",train1_df.shape)\nprint(train1_df.head)\nprint(\"test1\",test1_df.shape)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:43:26.723185Z","iopub.execute_input":"2024-05-01T07:43:26.723607Z","iopub.status.idle":"2024-05-01T07:43:26.903392Z","shell.execute_reply.started":"2024-05-01T07:43:26.723573Z","shell.execute_reply":"2024-05-01T07:43:26.902358Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"train1 (6920, 2)\n<bound method NDFrame.head of                                                   tweet  \\\n0     @user Ignora al otro, es un capullo.El problem...   \n1     @user Si comicsgate se parece en algo a gamerg...   \n2     @user Lee sobre Gamergate, y como eso ha cambi...   \n3     @user Un retraso social bastante lamentable, g...   \n4     @user @user @user Entonces como así es el merc...   \n...                                                 ...   \n6915  idk why y’all bitches think having half your a...   \n6916  This has been a part of an experiment with @us...   \n6917  \"Take me already\" \"Not yet. You gotta be ready...   \n6918       @user why do you look like a whore? /lh http   \n6919  ik when mandy says “you look like a whore” i l...   \n\n                                     soft_label  \n0      [0.833333333333333, 0.16666666666666602]  \n1      [0.16666666666666602, 0.833333333333333]  \n2                                    [0.0, 1.0]  \n3                                    [0.5, 0.5]  \n4     [0.6666666666666661, 0.33333333333333304]  \n...                                         ...  \n6915                                 [1.0, 0.0]  \n6916                                 [1.0, 0.0]  \n6917  [0.6666666666666661, 0.33333333333333304]  \n6918                                 [1.0, 0.0]  \n6919   [0.833333333333333, 0.16666666666666602]  \n\n[6920 rows x 2 columns]>\ntest1 (2076, 1)\n<bound method NDFrame.head of                                                   tweet\n0     @user Todo gamergate desde el desarrollo hasta...\n1     @user @user Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @user Primero fue internet, luego el gamergate...\n4     @user Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @user This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @user @user You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:43:32.024054Z","iopub.execute_input":"2024-05-01T07:43:32.024450Z","iopub.status.idle":"2024-05-01T07:43:32.039067Z","shell.execute_reply.started":"2024-05-01T07:43:32.024419Z","shell.execute_reply":"2024-05-01T07:43:32.037947Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-uncased\", num_labels=2,ignore_mismatched_sizes=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:19:15.309694Z","iopub.execute_input":"2024-05-01T06:19:15.310368Z","iopub.status.idle":"2024-05-01T06:19:17.944924Z","shell.execute_reply.started":"2024-05-01T06:19:15.310337Z","shell.execute_reply":"2024-05-01T06:19:17.943922Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# with lr=2e-5","metadata":{}},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 128\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/best-model/soft/\",\n    report_to=None,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:19:53.529508Z","iopub.execute_input":"2024-05-01T06:19:53.529922Z","iopub.status.idle":"2024-05-01T06:19:53.564961Z","shell.execute_reply.started":"2024-05-01T06:19:53.529891Z","shell.execute_reply":"2024-05-01T06:19:53.564078Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# with lr=3e-5","metadata":{}},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 128\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/best-model/soft/\",\n    report_to=None,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:43:44.380344Z","iopub.execute_input":"2024-05-01T07:43:44.381040Z","iopub.status.idle":"2024-05-01T07:43:44.418926Z","shell.execute_reply.started":"2024-05-01T07:43:44.381005Z","shell.execute_reply":"2024-05-01T07:43:44.417860Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\ndef convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:43:47.960229Z","iopub.execute_input":"2024-05-01T07:43:47.960610Z","iopub.status.idle":"2024-05-01T07:43:47.966872Z","shell.execute_reply.started":"2024-05-01T07:43:47.960579Z","shell.execute_reply":"2024-05-01T07:43:47.965824Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:43:50.441670Z","iopub.execute_input":"2024-05-01T07:43:50.442061Z","iopub.status.idle":"2024-05-01T07:43:52.348217Z","shell.execute_reply.started":"2024-05-01T07:43:50.442031Z","shell.execute_reply":"2024-05-01T07:43:52.347141Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def custom_loss_fn(logits, soft_labels):\n    probs = F.softmax(logits, dim=1)\n    # Apply nn.CrossEntropyLoss\n    loss = nn.CrossEntropyLoss(reduction=\"sum\",label_smoothing=0.15)(probs, soft_labels)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:20:05.162077Z","iopub.execute_input":"2024-05-01T06:20:05.162483Z","iopub.status.idle":"2024-05-01T06:20:05.167615Z","shell.execute_reply.started":"2024-05-01T06:20:05.162452Z","shell.execute_reply":"2024-05-01T06:20:05.166615Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# with lr=2e-5","metadata":{}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = custom_loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# Define the trainer for each fold\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:20:09.488843Z","iopub.execute_input":"2024-05-01T06:20:09.489899Z","iopub.status.idle":"2024-05-01T06:33:46.625859Z","shell.execute_reply.started":"2024-05-01T06:20:09.489862Z","shell.execute_reply":"2024-05-01T06:33:46.624820Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2115' max='2115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2115/2115 13:35, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>10.089100</td>\n      <td>19.161743</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>9.503400</td>\n      <td>19.068266</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>9.200000</td>\n      <td>19.000187</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>9.014700</td>\n      <td>18.973082</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2115, training_loss=9.341863595227542, metrics={'train_runtime': 816.0835, 'train_samples_per_second': 41.442, 'train_steps_per_second': 2.592, 'total_flos': 2224603973068800.0, 'train_loss': 9.341863595227542, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# with lr=3e-5","metadata":{}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = custom_loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# Define the trainer for each fold\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:43:54.799693Z","iopub.execute_input":"2024-05-01T07:43:54.800587Z","iopub.status.idle":"2024-05-01T07:57:32.381569Z","shell.execute_reply.started":"2024-05-01T07:43:54.800550Z","shell.execute_reply":"2024-05-01T07:57:32.380526Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2115' max='2115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2115/2115 13:36, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>9.301600</td>\n      <td>18.027428</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>8.903200</td>\n      <td>18.022764</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>8.802900</td>\n      <td>17.967220</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>8.740700</td>\n      <td>17.960915</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2115, training_loss=8.887947071697695, metrics={'train_runtime': 816.7203, 'train_samples_per_second': 41.41, 'train_steps_per_second': 2.59, 'total_flos': 2224603973068800.0, 'train_loss': 8.887947071697695, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# with lr=2e-5","metadata":{}},{"cell_type":"code","source":"\nfrom scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_logit_labels = val_predictions.predictions\nval_pred_probabilities = softmax(val_pred_logit_labels, axis=1)\n\nval_pred_hard_labels = np.argmax(val_pred_probabilities, axis=1)\nval_true_labels = val_dataset[\"label\"]\nval_true_hard_labels = np.argmax(val_dataset[\"label\"],axis=1)\n\n# Calculate evaluation metrics for each fold\nval_accuracy = accuracy_score(val_true_hard_labels, val_pred_hard_labels)\nval_f1_score = f1_score(val_true_hard_labels, val_pred_hard_labels,average='weighted')\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:37:15.297019Z","iopub.execute_input":"2024-05-01T06:37:15.297948Z","iopub.status.idle":"2024-05-01T06:37:22.354621Z","shell.execute_reply.started":"2024-05-01T06:37:15.297912Z","shell.execute_reply":"2024-05-01T06:37:22.353641Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.821608040201005\nValidation F1 Score: 0.821529522778418\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# with lr=3e-5","metadata":{}},{"cell_type":"code","source":"\nfrom scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_logit_labels = val_predictions.predictions\nval_pred_probabilities = softmax(val_pred_logit_labels, axis=1)\n\nval_pred_hard_labels = np.argmax(val_pred_probabilities, axis=1)\nval_true_labels = val_dataset[\"label\"]\nval_true_hard_labels = np.argmax(val_dataset[\"label\"],axis=1)\n\n# Calculate evaluation metrics for each fold\nval_accuracy = accuracy_score(val_true_hard_labels, val_pred_hard_labels)\nval_f1_score = f1_score(val_true_hard_labels, val_pred_hard_labels,average='weighted')\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:03:04.517813Z","iopub.execute_input":"2024-05-01T08:03:04.518550Z","iopub.status.idle":"2024-05-01T08:03:11.585277Z","shell.execute_reply.started":"2024-05-01T08:03:04.518508Z","shell.execute_reply":"2024-05-01T08:03:11.584306Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.9128978224455612\nValidation F1 Score: 0.9126874946611928\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions on the test set\nfrom scipy.special import softmax\ntest_predictions1 = trainer.predict(test_dataset)\ntest_pred_labels1 = np.argmax(test_predictions1.predictions, axis=1)\ntest_pred_probs1 = softmax(test_predictions1.predictions).tolist()\n\n#get probabilities to a list\ntest_probs_list1 = [] \nfor logits in test_pred_probs1:\n    test_probs_list1.append({'YES': logits[0], 'NO': logits[1]})","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:03:19.287571Z","iopub.execute_input":"2024-05-01T08:03:19.287965Z","iopub.status.idle":"2024-05-01T08:03:31.523420Z","shell.execute_reply.started":"2024-05-01T08:03:19.287935Z","shell.execute_reply":"2024-05-01T08:03:31.522295Z"},"trusted":true},"execution_count":68,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id\"].astype(int)\n    output_dict = {}\n    decoded_labels = task1_hard_decode(df)[\"hard_label\"].tolist()\n    print(\"decoded labels length:\",len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = decoded_labels[i]\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"task1_NICA-soft-3.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(test_pred_labels,columns=[\"hard_label\"]), test_probs_list1, og_test1[[\"id\"]],og_test1)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:03:39.139791Z","iopub.execute_input":"2024-05-01T08:03:39.140184Z","iopub.status.idle":"2024-05-01T08:03:39.226267Z","shell.execute_reply.started":"2024-05-01T08:03:39.140151Z","shell.execute_reply":"2024-05-01T08:03:39.225246Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"decoded labels length: 2076\noutput generated as json\n","output_type":"stream"}]}]}