{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8242355,"sourceType":"datasetVersion","datasetId":4889218}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-03T11:10:20.671344Z","iopub.execute_input":"2024-05-03T11:10:20.671788Z","iopub.status.idle":"2024-05-03T11:10:20.689865Z","shell.execute_reply.started":"2024-05-03T11:10:20.671750Z","shell.execute_reply":"2024-05-03T11:10:20.688937Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/train_data_task1.csv\n/kaggle/input/dev_data_task3.csv\n/kaggle/input/train_data_task2.csv\n/kaggle/input/dev_data_task1.csv\n/kaggle/input/train_data_task3.csv\n/kaggle/input/EXIT2024_tweet_test.csv\n/kaggle/input/dev_data_task2.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb offline\n!wandb disabled","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:10:23.356483Z","iopub.execute_input":"2024-05-03T11:10:23.357096Z","iopub.status.idle":"2024-05-03T11:10:26.933623Z","shell.execute_reply.started":"2024-05-03T11:10:23.357067Z","shell.execute_reply":"2024-05-03T11:10:26.932628Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\nW&B disabled.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:10:26.935922Z","iopub.execute_input":"2024-05-03T11:10:26.936335Z","iopub.status.idle":"2024-05-03T11:10:52.422567Z","shell.execute_reply.started":"2024-05-03T11:10:26.936297Z","shell.execute_reply":"2024-05-03T11:10:52.421545Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from  IPython.display import clear_output\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport pandas as pd\nimport numpy as np\nimport random\nimport  matplotlib. pyplot  as  plt\nfrom tqdm import tqdm\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:10:52.424429Z","iopub.execute_input":"2024-05-03T11:10:52.424730Z","iopub.status.idle":"2024-05-03T11:11:11.894944Z","shell.execute_reply.started":"2024-05-03T11:10:52.424702Z","shell.execute_reply":"2024-05-03T11:11:11.894109Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-05-03 11:11:02.584903: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-03 11:11:02.585006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-03 11:11:02.761824: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/train_data_task3.csv')\ndev_df = pd.read_csv('/kaggle/input/dev_data_task3.csv')\ntest_df = pd.read_csv('/kaggle/input/EXIT2024_tweet_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:11:11.896117Z","iopub.execute_input":"2024-05-03T11:11:11.896697Z","iopub.status.idle":"2024-05-03T11:11:12.070768Z","shell.execute_reply.started":"2024-05-03T11:11:11.896669Z","shell.execute_reply":"2024-05-03T11:11:12.069670Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef print_custom(text):\n    print('\\n')\n    print(text)\n    print('-'*100)\n\n    \ndef simple_preprocess(text):\n    \"\"\"\n    pass the tweet data as a series. do not use apply function\n    only preprocesses for replacing @USER and URLS\n    \"\"\"\n    # print(\"i am preprocessing\")\n    URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n    HANDLE_RE = re.compile(r\"@\\w+\")\n    tweets = []\n    for t in text:\n        t = HANDLE_RE.sub(\"@USER\", t)\n        t = URL_RE.sub(\"HTTPURL\", t)\n        tweets.append(t)\n    return tweets","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:11:12.072753Z","iopub.execute_input":"2024-05-03T11:11:12.073050Z","iopub.status.idle":"2024-05-03T11:11:12.079671Z","shell.execute_reply.started":"2024-05-03T11:11:12.073024Z","shell.execute_reply":"2024-05-03T11:11:12.078576Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\n# Instantiate multi-label encoder\ntask_encoder = MultiLabelBinarizer()\n\n   \ndef task_hard_encode(df):\n    import numpy as np\n    # Create a new DataFrame to store the transformed labels\n    transformed_df = df.copy()\n    task_encoder.fit(all_task_hard_labels)\n    all_labels = []\n    for i in range(len(df['hard_label'])):\n        # Apply eval to each element of the label column\n        all_labels.append(eval(df['hard_label'][i]))\n    \n    transformed_labels = task_encoder.transform(all_labels)\n    # print(pd.DataFrame(transformed_labels))\n    # transformed_df[\"hard_label\"] = pd.DataFrame(transformed_labels)\n    transformed_labels = pd.Series([x.tolist() for x in transformed_labels])\n    transformed_df[\"hard_label\"] = transformed_labels\n    return transformed_df\n\ndef task_hard_decode(df):\n    # Inverse transform the binary-encoded vectors back to original labels\n    decoded_df = df.copy()\n    decode_labels = np.array(decoded_df[\"hard_label\"].tolist())\n\n    # Inverse transform the label column using the provided task_encoder\n    decoded_labels = task_encoder.inverse_transform(decode_labels)\n    \n    # Assign the decoded labels back to the DataFrame\n    decoded_df[\"hard_labels\"] = decoded_labels\n\n    return decoded_df","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:11:12.080759Z","iopub.execute_input":"2024-05-03T11:11:12.081010Z","iopub.status.idle":"2024-05-03T11:11:12.090929Z","shell.execute_reply.started":"2024-05-03T11:11:12.080988Z","shell.execute_reply":"2024-05-03T11:11:12.090128Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \nimport os.path\nfrom os import path\n\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments,\\\n                        Trainer\nfrom datasets import Dataset\nimport pandas as pd\n\nog_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\ntrain1_df = og_train1[[\"tweet\",\"hard_label\"]].dropna().reset_index(drop=True)\ndev1_df = og_dev1[[\"tweet\",\"hard_label\"]].dropna().reset_index(drop=True)\n\nall_task_hard_labels = pd.concat([og_train1[\"hard_label\"],og_dev1[\"hard_label\"]]).dropna()\nall_task_hard_labels = all_task_hard_labels.apply(lambda x: eval(x)).tolist()\nprint(all_task_hard_labels[0])\n\ntrain1_df = task_hard_encode(train1_df)\n\ntrain1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\ntrain1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n\ndev1_df = task_hard_encode(dev1_df)\ndev1_df[\"tweet\"] = simple_preprocess(dev1_df[\"tweet\"])\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\nprint(\"train1\",train1_df.shape)\nprint(train1_df.head)\nprint(\"test1\",test1_df.shape)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:11:12.091952Z","iopub.execute_input":"2024-05-03T11:11:12.092254Z","iopub.status.idle":"2024-05-03T11:11:12.437744Z","shell.execute_reply.started":"2024-05-03T11:11:12.092231Z","shell.execute_reply":"2024-05-03T11:11:12.436745Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['OBJECTIFICATION', 'SEXUAL-VIOLENCE']\ntrain1 (6050, 2)\n<bound method NDFrame.head of                                                   tweet          hard_label\n0     @USER Ignora al otro, es un capullo.El problem...  [0, 0, 0, 1, 1, 0]\n1     @USER Si comicsgate se parece en algo a gamerg...  [0, 0, 1, 0, 0, 0]\n2     @USER Lee sobre Gamergate, y como eso ha cambi...  [0, 0, 1, 0, 0, 0]\n3     @USER @USER @USER Entonces como así es el merc...  [1, 0, 0, 1, 0, 1]\n4     @USER Aaah sí. Andrew Dobson. El que se dedicó...  [0, 0, 1, 0, 0, 0]\n...                                                 ...                 ...\n6045  idk why y’all bitches think having half your a...  [0, 1, 0, 1, 1, 1]\n6046  This has been a part of an experiment with @US...  [0, 0, 0, 1, 0, 0]\n6047  \"Take me already\" \"Not yet. You gotta be ready...  [0, 0, 0, 0, 1, 0]\n6048    @USER why do you look like a whore? /lh HTTPURL  [0, 1, 0, 1, 1, 1]\n6049  ik when mandy says “you look like a whore” i l...  [0, 0, 0, 1, 0, 1]\n\n[6050 rows x 2 columns]>\ntest1 (2076, 1)\n<bound method NDFrame.head of                                                   tweet\n0     @USER Todo gamergate desde el desarrollo hasta...\n1     @USER @USER Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @USER Primero fue internet, luego el gamergate...\n4     @USER Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @USER This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @USER @USER You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"train1_df.head","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:11:12.439039Z","iopub.execute_input":"2024-05-03T11:11:12.439366Z","iopub.status.idle":"2024-05-03T11:11:12.450829Z","shell.execute_reply.started":"2024-05-03T11:11:12.439340Z","shell.execute_reply":"2024-05-03T11:11:12.449822Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of                                                   tweet          hard_label\n0     @USER Ignora al otro, es un capullo.El problem...  [0, 0, 0, 1, 1, 0]\n1     @USER Si comicsgate se parece en algo a gamerg...  [0, 0, 1, 0, 0, 0]\n2     @USER Lee sobre Gamergate, y como eso ha cambi...  [0, 0, 1, 0, 0, 0]\n3     @USER @USER @USER Entonces como así es el merc...  [1, 0, 0, 1, 0, 1]\n4     @USER Aaah sí. Andrew Dobson. El que se dedicó...  [0, 0, 1, 0, 0, 0]\n...                                                 ...                 ...\n6045  idk why y’all bitches think having half your a...  [0, 1, 0, 1, 1, 1]\n6046  This has been a part of an experiment with @US...  [0, 0, 0, 1, 0, 0]\n6047  \"Take me already\" \"Not yet. You gotta be ready...  [0, 0, 0, 0, 1, 0]\n6048    @USER why do you look like a whore? /lh HTTPURL  [0, 1, 0, 1, 1, 1]\n6049  ik when mandy says “you look like a whore” i l...  [0, 0, 0, 1, 0, 1]\n\n[6050 rows x 2 columns]>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:11:21.653882Z","iopub.execute_input":"2024-05-03T11:11:21.654262Z","iopub.status.idle":"2024-05-03T11:11:21.684058Z","shell.execute_reply.started":"2024-05-03T11:11:21.654230Z","shell.execute_reply":"2024-05-03T11:11:21.682922Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer and model\nmodel_name = \"bert-base-multilingual-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6,ignore_mismatched_sizes=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:12:15.136838Z","iopub.execute_input":"2024-05-03T11:12:15.137896Z","iopub.status.idle":"2024-05-03T11:12:20.533975Z","shell.execute_reply.started":"2024-05-03T11:12:15.137857Z","shell.execute_reply":"2024-05-03T11:12:20.533182Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"451e018b29494560b889cfce92b7e785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b899b481114535b8a82717ae6fc86a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2a14f490f240ed8e8605b81ab9a205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58aa4f6834c14777ba7c2e0676c74003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98501bca8e2640608e7503d4a81018fe"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 512\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    # accelerator = Accelerator(),\n    output_dir=\"./output/best-model/taks3/hard\",\n    num_train_epochs=4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    learning_rate= 1e-5, \n    weight_decay= 0.01, \n    logging_dir=\"./logs\",\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:12:49.559824Z","iopub.execute_input":"2024-05-03T11:12:49.560666Z","iopub.status.idle":"2024-05-03T11:12:49.666957Z","shell.execute_reply.started":"2024-05-03T11:12:49.560635Z","shell.execute_reply":"2024-05-03T11:12:49.666189Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:12:54.160703Z","iopub.execute_input":"2024-05-03T11:12:54.161072Z","iopub.status.idle":"2024-05-03T11:12:54.166869Z","shell.execute_reply.started":"2024-05-03T11:12:54.161043Z","shell.execute_reply":"2024-05-03T11:12:54.165942Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:12:56.361134Z","iopub.execute_input":"2024-05-03T11:12:56.361521Z","iopub.status.idle":"2024-05-03T11:13:00.294108Z","shell.execute_reply.started":"2024-05-03T11:12:56.361493Z","shell.execute_reply":"2024-05-03T11:13:00.293091Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define the trainer for each fold\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model for each fold .39\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:13:02.857099Z","iopub.execute_input":"2024-05-03T11:13:02.858108Z","iopub.status.idle":"2024-05-03T11:35:48.571806Z","shell.execute_reply.started":"2024-05-03T11:13:02.858069Z","shell.execute_reply":"2024-05-03T11:35:48.570756Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1484' max='1484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1484/1484 22:38, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.468500</td>\n      <td>0.393863</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.369200</td>\n      <td>0.363385</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1484, training_loss=0.3635362846189432, metrics={'train_runtime': 1364.3263, 'train_samples_per_second': 17.395, 'train_steps_per_second': 1.088, 'total_flos': 6244375820673024.0, 'train_loss': 0.3635362846189432, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Get predictions on the validation set\nfrom scipy.special import expit\nval_predictions = trainer.predict(val_dataset)\nval_pred_probabilities = expit(val_predictions.predictions)\n\nval_pred_labels = np.where(val_pred_probabilities > 0.5, 1, 0)\nval_true_labels = val_dataset[\"label\"]\n\n# Calculate evaluation metrics \nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels,average = \"weighted\")\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:35:48.573415Z","iopub.execute_input":"2024-05-03T11:35:48.574050Z","iopub.status.idle":"2024-05-03T11:36:09.928618Z","shell.execute_reply.started":"2024-05-03T11:35:48.574022Z","shell.execute_reply":"2024-05-03T11:36:09.927667Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.5133587786259542\nValidation F1 Score: 0.5849003123954302\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions on the test set\nfrom scipy.special import expit\ntest_predictions = trainer.predict(test_dataset)\ntest_pred_probabilities = expit(test_predictions.predictions) #same as sigmoid\ntest_pred_labels = np.where(test_pred_probabilities > 0.5, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:36:09.933880Z","iopub.execute_input":"2024-05-03T11:36:09.934181Z","iopub.status.idle":"2024-05-03T11:36:52.105538Z","shell.execute_reply.started":"2024-05-03T11:36:09.934134Z","shell.execute_reply":"2024-05-03T11:36:52.104553Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"def create_results(probabilities, class_labels):\n    results = []\n    for prob_list in probabilities:\n        result_dict = {label: prob for label, prob in zip(class_labels, prob_list)}\n        results.append(result_dict)\n    return results\n\ntest_probs_list = create_results(test_pred_probabilities.tolist(), task_encoder.classes_)\ntest_probs_list[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:36:52.106759Z","iopub.execute_input":"2024-05-03T11:36:52.107082Z","iopub.status.idle":"2024-05-03T11:36:52.124541Z","shell.execute_reply.started":"2024-05-03T11:36:52.107055Z","shell.execute_reply":"2024-05-03T11:36:52.123588Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[{'IDEOLOGICAL-INEQUALITY': 0.054431889206171036,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.040640994906425476,\n  'NO': 0.9417867064476013,\n  'OBJECTIFICATION': 0.03226807340979576,\n  'SEXUAL-VIOLENCE': 0.04577834531664848,\n  'STEREOTYPING-DOMINANCE': 0.04994960501790047},\n {'IDEOLOGICAL-INEQUALITY': 0.13054528832435608,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.0377262681722641,\n  'NO': 0.8963083624839783,\n  'OBJECTIFICATION': 0.026097968220710754,\n  'SEXUAL-VIOLENCE': 0.034643396735191345,\n  'STEREOTYPING-DOMINANCE': 0.07548177242279053},\n {'IDEOLOGICAL-INEQUALITY': 0.04679815098643303,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.03858187794685364,\n  'NO': 0.9424629211425781,\n  'OBJECTIFICATION': 0.03298630565404892,\n  'SEXUAL-VIOLENCE': 0.045080289244651794,\n  'STEREOTYPING-DOMINANCE': 0.04796266555786133},\n {'IDEOLOGICAL-INEQUALITY': 0.49032238125801086,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.17240697145462036,\n  'NO': 0.17218567430973053,\n  'OBJECTIFICATION': 0.11749958246946335,\n  'SEXUAL-VIOLENCE': 0.08719509094953537,\n  'STEREOTYPING-DOMINANCE': 0.30818265676498413},\n {'IDEOLOGICAL-INEQUALITY': 0.0494726188480854,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.036202073097229004,\n  'NO': 0.9352399110794067,\n  'OBJECTIFICATION': 0.031757939606904984,\n  'SEXUAL-VIOLENCE': 0.04147005081176758,\n  'STEREOTYPING-DOMINANCE': 0.048506904393434525},\n {'IDEOLOGICAL-INEQUALITY': 0.047112490981817245,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.03929165005683899,\n  'NO': 0.9418472647666931,\n  'OBJECTIFICATION': 0.0332597941160202,\n  'SEXUAL-VIOLENCE': 0.0461416132748127,\n  'STEREOTYPING-DOMINANCE': 0.047072719782590866},\n {'IDEOLOGICAL-INEQUALITY': 0.206403449177742,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.11386100202798843,\n  'NO': 0.3344361186027527,\n  'OBJECTIFICATION': 0.11633536219596863,\n  'SEXUAL-VIOLENCE': 0.09666537493467331,\n  'STEREOTYPING-DOMINANCE': 0.20856094360351562},\n {'IDEOLOGICAL-INEQUALITY': 0.06258014589548111,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.04650752618908882,\n  'NO': 0.8788446187973022,\n  'OBJECTIFICATION': 0.042242031544446945,\n  'SEXUAL-VIOLENCE': 0.039266981184482574,\n  'STEREOTYPING-DOMINANCE': 0.06736066192388535},\n {'IDEOLOGICAL-INEQUALITY': 0.049380961805582047,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.0410609245300293,\n  'NO': 0.9415094256401062,\n  'OBJECTIFICATION': 0.034477509558200836,\n  'SEXUAL-VIOLENCE': 0.04763106256723404,\n  'STEREOTYPING-DOMINANCE': 0.047667086124420166},\n {'IDEOLOGICAL-INEQUALITY': 0.03208937495946884,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.03886155039072037,\n  'NO': 0.8920592665672302,\n  'OBJECTIFICATION': 0.04825957864522934,\n  'SEXUAL-VIOLENCE': 0.059798408299684525,\n  'STEREOTYPING-DOMINANCE': 0.05125516280531883}]"},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id\"].astype(int)\n    output_dict = {}\n    decoded_labels = task_hard_decode(df)[\"hard_labels\"].tolist()\n    print(\"decoded labels length:\", len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = list(decoded_labels[i])\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"./output/task3_NICA-hard-2.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(pd.Series(test_pred_labels.tolist()), columns=[\"hard_label\"]), test_probs_list, og_test1[[\"id\"]],og_test1)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:36:52.125737Z","iopub.execute_input":"2024-05-03T11:36:52.125994Z","iopub.status.idle":"2024-05-03T11:36:52.250848Z","shell.execute_reply.started":"2024-05-03T11:36:52.125971Z","shell.execute_reply":"2024-05-03T11:36:52.250001Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"decoded labels length: 2076\noutput generated as json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Soft","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/train_data_task3.csv')\ndev_df = pd.read_csv('/kaggle/input/dev_data_task3.csv')\ntest_df = pd.read_csv('/kaggle/input/EXIT2024_tweet_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:41:11.832056Z","iopub.execute_input":"2024-05-03T11:41:11.833078Z","iopub.status.idle":"2024-05-03T11:41:11.916050Z","shell.execute_reply.started":"2024-05-03T11:41:11.833038Z","shell.execute_reply":"2024-05-03T11:41:11.915161Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#relevant functions\nimport re\n# Create function for printing \ndef print_custom(text):\n    print('\\n')\n    print(text)\n    print('-'*100)\n    \ndef simple_preprocess(text):\n    \"\"\"\n    pass the tweet data as a series. do not use apply function\n    only preprocesses for replacing @USER and URLS\n    \"\"\"\n    # print(\"i am preprocessing\")\n    URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n    HANDLE_RE = re.compile(r\"@\\w+\")\n    tweets = []\n    for t in text:\n        t = HANDLE_RE.sub(\"@USER\", t)\n        t = URL_RE.sub(\"HTTPURL\", t)\n        tweets.append(t)\n    return tweets\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndef convert_logits_to_list(logits_dict):\n    \"\"\"\n    order:\n    1. classes are \n    OBJECTIFICATION,'SEXUAL-VIOLENCE',NO,STEREOTYPING-DOMINANCE,IDEOLOGICAL-INEQUALITY,MISOGYNY-NON-SEXUAL-VIOLENCE\n    \"\"\"\n    logits_dict = eval(logits_dict)\n    logits_list = [logits_dict[\"OBJECTIFICATION\"], logits_dict[\"SEXUAL-VIOLENCE\"],logits_dict[\"NO\"],logits_dict[\"STEREOTYPING-DOMINANCE\"],logits_dict[\"IDEOLOGICAL-INEQUALITY\"], logits_dict[\"MISOGYNY-NON-SEXUAL-VIOLENCE\"]]\n    return logits_list\n\ndef convert_list_to_logits(logits_list):\n    logits_dict = {\"OBJECTIFICATION\": logits_list[0], \"SEXUAL-VIOLENCE\": logits_list[1],\"NO\": logits_list[2], \"STEREOTYPING-DOMINANCE\": logits_list[3],\"IDEOLOGICAL-INEQUALITY\": logits_list[4], \"MISOGYNY-NON-SEXUAL-VIOLENCE\": logits_list[5]}\n    return logits_dict\n\ndef check_dtype(given_data):\n    return eval(given_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:41:15.144765Z","iopub.execute_input":"2024-05-03T11:41:15.145664Z","iopub.status.idle":"2024-05-03T11:41:15.155068Z","shell.execute_reply.started":"2024-05-03T11:41:15.145628Z","shell.execute_reply":"2024-05-03T11:41:15.154187Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer  \nimport os.path\nfrom os import path\n\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\nfrom datasets import Dataset\n\n\n#load given data\nimport pandas as pd\nog_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\nog_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\nog_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\ntrain1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n\n\nog_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\nog_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\ndev1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\n\nprint(train1_df.head)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:41:19.122008Z","iopub.execute_input":"2024-05-03T11:41:19.122405Z","iopub.status.idle":"2024-05-03T11:41:19.403585Z","shell.execute_reply.started":"2024-05-03T11:41:19.122375Z","shell.execute_reply":"2024-05-03T11:41:19.402656Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"<bound method NDFrame.head of                                                   tweet  \\\n0     @USER Ignora al otro, es un capullo.El problem...   \n1     @USER Si comicsgate se parece en algo a gamerg...   \n2     @USER Lee sobre Gamergate, y como eso ha cambi...   \n3     @USER Un retraso social bastante lamentable, g...   \n4     @USER @USER @USER Entonces como así es el merc...   \n...                                                 ...   \n6915  idk why y’all bitches think having half your a...   \n6916  This has been a part of an experiment with @US...   \n6917  \"Take me already\" \"Not yet. You gotta be ready...   \n6918    @USER why do you look like a whore? /lh HTTPURL   \n6919  ik when mandy says “you look like a whore” i l...   \n\n                                             soft_label  \n0     [0.33333333333333304, 0.33333333333333304, 0.1...  \n1     [0.16666666666666602, 0.0, 0.833333333333333, ...  \n2                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n3     [0.16666666666666602, 0.16666666666666602, 0.5...  \n4     [0.5, 0.0, 0.33333333333333304, 0.5, 0.3333333...  \n...                                                 ...  \n6915  [0.6666666666666661, 0.6666666666666661, 0.0, ...  \n6916  [0.6666666666666661, 0.0, 0.0, 0.1666666666666...  \n6917  [0.16666666666666602, 0.33333333333333304, 0.3...  \n6918  [0.6666666666666661, 0.5, 0.0, 0.3333333333333...  \n6919  [0.5, 0.16666666666666602, 0.16666666666666602...  \n\n[6920 rows x 2 columns]>\n<bound method NDFrame.head of                                                   tweet\n0     @USER Todo gamergate desde el desarrollo hasta...\n1     @USER @USER Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @USER Primero fue internet, luego el gamergate...\n4     @USER Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @USER This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @USER @USER You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"train1_df.info","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:58:40.839198Z","iopub.execute_input":"2024-04-27T14:58:40.839940Z","iopub.status.idle":"2024-04-27T14:58:40.850569Z","shell.execute_reply.started":"2024-04-27T14:58:40.839909Z","shell.execute_reply":"2024-04-27T14:58:40.849613Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<bound method DataFrame.info of                                                   tweet  \\\n0     @USER Ignora al otro, es un capullo.El problem...   \n1     @USER Si comicsgate se parece en algo a gamerg...   \n2     @USER Lee sobre Gamergate, y como eso ha cambi...   \n3     @USER Un retraso social bastante lamentable, g...   \n4     @USER @USER @USER Entonces como así es el merc...   \n...                                                 ...   \n6915  idk why y’all bitches think having half your a...   \n6916  This has been a part of an experiment with @US...   \n6917  \"Take me already\" \"Not yet. You gotta be ready...   \n6918    @USER why do you look like a whore? /lh HTTPURL   \n6919  ik when mandy says “you look like a whore” i l...   \n\n                                             soft_label  \n0     [0.33333333333333304, 0.33333333333333304, 0.1...  \n1     [0.16666666666666602, 0.0, 0.833333333333333, ...  \n2                        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n3     [0.16666666666666602, 0.16666666666666602, 0.5...  \n4     [0.5, 0.0, 0.33333333333333304, 0.5, 0.3333333...  \n...                                                 ...  \n6915  [0.6666666666666661, 0.6666666666666661, 0.0, ...  \n6916  [0.6666666666666661, 0.0, 0.0, 0.1666666666666...  \n6917  [0.16666666666666602, 0.33333333333333304, 0.3...  \n6918  [0.6666666666666661, 0.5, 0.0, 0.3333333333333...  \n6919  [0.5, 0.16666666666666602, 0.16666666666666602...  \n\n[6920 rows x 2 columns]>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:41:24.584632Z","iopub.execute_input":"2024-05-03T11:41:24.585006Z","iopub.status.idle":"2024-05-03T11:41:24.599569Z","shell.execute_reply.started":"2024-05-03T11:41:24.584976Z","shell.execute_reply":"2024-05-03T11:41:24.598412Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer and model\nmodel_name = \"bert-base-multilingual-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6,ignore_mismatched_sizes=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:41:37.124588Z","iopub.execute_input":"2024-05-03T11:41:37.124949Z","iopub.status.idle":"2024-05-03T11:41:39.436851Z","shell.execute_reply.started":"2024-05-03T11:41:37.124921Z","shell.execute_reply":"2024-05-03T11:41:39.436039Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 512\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/task3/soft\",\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    learning_rate= 1e-5, \n    weight_decay= 0.01, \n    logging_dir=\"./logs\",\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:41:59.184506Z","iopub.execute_input":"2024-05-03T11:41:59.184895Z","iopub.status.idle":"2024-05-03T11:41:59.218082Z","shell.execute_reply.started":"2024-05-03T11:41:59.184865Z","shell.execute_reply":"2024-05-03T11:41:59.217358Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\ndef convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\n# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:42:02.139632Z","iopub.execute_input":"2024-05-03T11:42:02.140598Z","iopub.status.idle":"2024-05-03T11:42:06.322759Z","shell.execute_reply.started":"2024-05-03T11:42:02.140562Z","shell.execute_reply":"2024-05-03T11:42:06.321708Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#with cross entropy loss and label smoothing\ndef custom_loss_fn(logits, soft_labels):\n    probs = torch.sigmoid(logits)\n    # Apply nn.CrossEntropyLoss\n    loss = nn.CrossEntropyLoss(reduction=\"sum\",label_smoothing=0.15)(probs, soft_labels)\n    return loss\n\n# Define the trainer for each fold\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = custom_loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:42:07.524419Z","iopub.execute_input":"2024-05-03T11:42:07.525085Z","iopub.status.idle":"2024-05-03T11:42:07.531426Z","shell.execute_reply.started":"2024-05-03T11:42:07.525040Z","shell.execute_reply":"2024-05-03T11:42:07.530404Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T11:42:15.094698Z","iopub.execute_input":"2024-05-03T11:42:15.095591Z","iopub.status.idle":"2024-05-03T12:15:18.833370Z","shell.execute_reply.started":"2024-05-03T11:42:15.095556Z","shell.execute_reply":"2024-05-03T12:15:18.832292Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2115' max='2115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2115/2115 33:01, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>29.664300</td>\n      <td>28.391594</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>28.790900</td>\n      <td>28.159575</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>28.433200</td>\n      <td>28.124615</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>28.180700</td>\n      <td>28.102289</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2115, training_loss=28.62005023640662, metrics={'train_runtime': 1982.515, 'train_samples_per_second': 17.059, 'train_steps_per_second': 1.067, 'total_flos': 8898735473418240.0, 'train_loss': 28.62005023640662, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom scipy.special import expit\n\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_probabilities = expit(val_predictions.predictions)\n\nval_pred_labels = np.where(val_pred_probabilities > 0.5, 1, 0)\nval_true_labels = [[1 if value > 0.5 else 0 for value in sublist] for sublist in val_dataset[\"label\"]]\n\n# Calculate evaluation metrics \nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels,average = \"weighted\")\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T12:15:18.836067Z","iopub.execute_input":"2024-05-03T12:15:18.836487Z","iopub.status.idle":"2024-05-03T12:15:43.145255Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.41792294807370184\nValidation F1 Score: 0.6129816564021449\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions on the test set\ntest_predictions1 = trainer.predict(test_dataset)\n\ntest_pred_probabilities1 = expit(test_predictions1.predictions) #, axis=1 #same as sigmoid\ntest_pred_labels1 = np.where(test_pred_probabilities1 > 0.5, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T12:15:43.154246Z","iopub.execute_input":"2024-05-03T12:15:43.154573Z","iopub.status.idle":"2024-05-03T12:16:25.752647Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"def create_results(probabilities, class_labels):\n    results = []\n    for prob_list in probabilities:\n        result_dict = {label: prob for label, prob in zip(class_labels, prob_list)}\n        results.append(result_dict)\n    return results\n\nclasses_labels = [\"OBJECTIFICATION\",\"SEXUAL-VIOLENCE\",\"NO\",\"STEREOTYPING-DOMINANCE\",\"IDEOLOGICAL-INEQUALITY\",\"MISOGYNY-NON-SEXUAL-VIOLENCE\"]\ntest_probs_list1 = create_results(test_pred_probabilities1.tolist(), classes_labels)\ntest_probs_list1[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-03T12:16:25.753898Z","iopub.execute_input":"2024-05-03T12:16:25.754222Z","iopub.status.idle":"2024-05-03T12:16:25.770745Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[{'OBJECTIFICATION': 0.018078025430440903,\n  'SEXUAL-VIOLENCE': 0.01837700791656971,\n  'NO': 0.99741131067276,\n  'STEREOTYPING-DOMINANCE': 0.021465227007865906,\n  'IDEOLOGICAL-INEQUALITY': 0.022991664707660675,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.011770358309149742},\n {'OBJECTIFICATION': 0.015279893763363361,\n  'SEXUAL-VIOLENCE': 0.006716907024383545,\n  'NO': 0.986621618270874,\n  'STEREOTYPING-DOMINANCE': 0.2801680266857147,\n  'IDEOLOGICAL-INEQUALITY': 0.6407381296157837,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.016466762870550156},\n {'OBJECTIFICATION': 0.018801532685756683,\n  'SEXUAL-VIOLENCE': 0.019930977374315262,\n  'NO': 0.9974833130836487,\n  'STEREOTYPING-DOMINANCE': 0.020479537546634674,\n  'IDEOLOGICAL-INEQUALITY': 0.01990232802927494,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.011894644238054752},\n {'OBJECTIFICATION': 0.009448055177927017,\n  'SEXUAL-VIOLENCE': 0.017346909269690514,\n  'NO': 0.9971910119056702,\n  'STEREOTYPING-DOMINANCE': 0.019741902127861977,\n  'IDEOLOGICAL-INEQUALITY': 0.05045255646109581,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.012888803146779537},\n {'OBJECTIFICATION': 0.015149527229368687,\n  'SEXUAL-VIOLENCE': 0.014691486954689026,\n  'NO': 0.9974727034568787,\n  'STEREOTYPING-DOMINANCE': 0.02476729080080986,\n  'IDEOLOGICAL-INEQUALITY': 0.032370876520872116,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.010269483551383018},\n {'OBJECTIFICATION': 0.017074182629585266,\n  'SEXUAL-VIOLENCE': 0.019047264009714127,\n  'NO': 0.9974639415740967,\n  'STEREOTYPING-DOMINANCE': 0.020625341683626175,\n  'IDEOLOGICAL-INEQUALITY': 0.02174375392496586,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.011850361712276936},\n {'OBJECTIFICATION': 0.02806836925446987,\n  'SEXUAL-VIOLENCE': 0.026514606550335884,\n  'NO': 0.9508131742477417,\n  'STEREOTYPING-DOMINANCE': 0.1911391317844391,\n  'IDEOLOGICAL-INEQUALITY': 0.37474262714385986,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.08638210594654083},\n {'OBJECTIFICATION': 0.03103771060705185,\n  'SEXUAL-VIOLENCE': 0.021604958921670914,\n  'NO': 0.9971933960914612,\n  'STEREOTYPING-DOMINANCE': 0.0319242961704731,\n  'IDEOLOGICAL-INEQUALITY': 0.012894167564809322,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.013560394756495953},\n {'OBJECTIFICATION': 0.015938078984618187,\n  'SEXUAL-VIOLENCE': 0.019290076568722725,\n  'NO': 0.9974404573440552,\n  'STEREOTYPING-DOMINANCE': 0.022391872480511665,\n  'IDEOLOGICAL-INEQUALITY': 0.024049120023846626,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.012338471598923206},\n {'OBJECTIFICATION': 0.02823764458298683,\n  'SEXUAL-VIOLENCE': 0.03597491979598999,\n  'NO': 0.9969095587730408,\n  'STEREOTYPING-DOMINANCE': 0.016034187749028206,\n  'IDEOLOGICAL-INEQUALITY': 0.012599948793649673,\n  'MISOGYNY-NON-SEXUAL-VIOLENCE': 0.016270797699689865}]"},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id\"].astype(int)\n    output_dict = {}\n    decoded_labels = task_hard_decode(df)[\"hard_labels\"].tolist()\n    print(\"decoded labels length:\",len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = list(decoded_labels[i])\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"./output/task3_NICA-soft-2.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(pd.Series(test_pred_labels1.tolist()),columns=[\"hard_label\"]), test_probs_list, og_test1[[\"id\"]],og_test1)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T12:16:43.292838Z","iopub.execute_input":"2024-05-03T12:16:43.293738Z","iopub.status.idle":"2024-05-03T12:16:43.416372Z","shell.execute_reply.started":"2024-05-03T12:16:43.293701Z","shell.execute_reply":"2024-05-03T12:16:43.415421Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"decoded labels length: 2076\noutput generated as json\n","output_type":"stream"}]}]}