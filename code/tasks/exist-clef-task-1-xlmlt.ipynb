{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8204367,"sourceType":"datasetVersion","datasetId":4860928}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-24T11:51:18.075773Z","iopub.execute_input":"2024-04-24T11:51:18.076147Z","iopub.status.idle":"2024-04-24T11:51:18.441309Z","shell.execute_reply.started":"2024-04-24T11:51:18.076115Z","shell.execute_reply":"2024-04-24T11:51:18.440420Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/exist-clef-2024-task-1/train_data_task1.csv\n/kaggle/input/exist-clef-2024-task-1/dev_data_task1.csv\n/kaggle/input/exist-clef-2024-task-1/EXIT2024_tweet_test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb offline\n!wandb disabled","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:51:21.021301Z","iopub.execute_input":"2024-04-24T11:51:21.022198Z","iopub.status.idle":"2024-04-24T11:51:25.533697Z","shell.execute_reply.started":"2024-04-24T11:51:21.022162Z","shell.execute_reply":"2024-04-24T11:51:25.532615Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\nW&B disabled.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:51:32.310324Z","iopub.execute_input":"2024-04-24T11:51:32.311247Z","iopub.status.idle":"2024-04-24T11:51:58.965287Z","shell.execute_reply.started":"2024-04-24T11:51:32.311210Z","shell.execute_reply":"2024-04-24T11:51:58.964359Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from  IPython. display import clear_output\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport pandas as pd\nimport numpy as np\nimport random\nimport  matplotlib. pyplot  as  plt\nfrom tqdm import tqdm\nimport torch\nimport torch.optim as optim\nimport torch. nn.functional as F\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:53:17.880401Z","iopub.execute_input":"2024-04-24T11:53:17.880802Z","iopub.status.idle":"2024-04-24T11:53:44.913142Z","shell.execute_reply.started":"2024-04-24T11:53:17.880765Z","shell.execute_reply":"2024-04-24T11:53:44.912277Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-04-24 11:53:32.688038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-24 11:53:32.688132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-24 11:53:32.941651: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/exist-clef-2024-task-1/train_data_task1.csv')\ntest_df = pd.read_csv('/kaggle/input/exist-clef-2024-task-1/EXIT2024_tweet_test.csv')\ndev_df = pd.read_csv('/kaggle/input/exist-clef-2024-task-1/dev_data_task1.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:54:02.170545Z","iopub.execute_input":"2024-04-24T11:54:02.170918Z","iopub.status.idle":"2024-04-24T11:54:02.229580Z","shell.execute_reply.started":"2024-04-24T11:54:02.170889Z","shell.execute_reply":"2024-04-24T11:54:02.228775Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T11:41:33.178413Z","iopub.execute_input":"2024-04-23T11:41:33.178741Z","iopub.status.idle":"2024-04-23T11:41:33.224887Z","shell.execute_reply.started":"2024-04-23T11:41:33.178715Z","shell.execute_reply":"2024-04-23T11:41:33.223989Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2076 entries, 0 to 2075\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      2076 non-null   int64 \n 1   lang    2076 non-null   object\n 2   tweet   2076 non-null   object\n 3   split   2076 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 65.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T11:41:46.830848Z","iopub.execute_input":"2024-04-23T11:41:46.831499Z","iopub.status.idle":"2024-04-23T11:41:46.847794Z","shell.execute_reply.started":"2024-04-23T11:41:46.831469Z","shell.execute_reply":"2024-04-23T11:41:46.846849Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"       id lang                                              tweet    split\n0  500001   es  @Eurogamer_es Todo gamergate desde el desarrol...  TEST_ES\n1  500002   es  @ArCaNgEl__23 @Benzenazi Hombre, no es compara...  TEST_ES\n2  500003   es  yo buscando las empresas metidas en el gamerga...  TEST_ES\n3  500004   es  @jordirico Primero fue internet, luego el game...  TEST_ES\n4  500005   es  @AlonsoQuijano12 Yo estuve metido en el gamerg...  TEST_ES","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>lang</th>\n      <th>tweet</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>500001</td>\n      <td>es</td>\n      <td>@Eurogamer_es Todo gamergate desde el desarrol...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>500002</td>\n      <td>es</td>\n      <td>@ArCaNgEl__23 @Benzenazi Hombre, no es compara...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500003</td>\n      <td>es</td>\n      <td>yo buscando las empresas metidas en el gamerga...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>500004</td>\n      <td>es</td>\n      <td>@jordirico Primero fue internet, luego el game...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500005</td>\n      <td>es</td>\n      <td>@AlonsoQuijano12 Yo estuve metido en el gamerg...</td>\n      <td>TEST_ES</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def simple_preprocess(text):\n    \"\"\"\n    pass the tweet data as a series. do not use apply function\n    only preprocesses for replacing @USER and URLS\n    \"\"\"\n    URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n    HANDLE_RE = re.compile(r\"@\\w+\")\n    tweets = []\n    for t in text:\n        t = HANDLE_RE.sub(\"@user\", t)\n        t = URL_RE.sub(\"http\", t)\n        tweets.append(t)\n    return tweets","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:54:05.000896Z","iopub.execute_input":"2024-04-24T11:54:05.001591Z","iopub.status.idle":"2024-04-24T11:54:05.007126Z","shell.execute_reply.started":"2024-04-24T11:54:05.001560Z","shell.execute_reply":"2024-04-24T11:54:05.006223Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport os.path\nfrom os import path\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\nfrom datasets import Dataset\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:54:06.542210Z","iopub.execute_input":"2024-04-24T11:54:06.542937Z","iopub.status.idle":"2024-04-24T11:54:06.582379Z","shell.execute_reply.started":"2024-04-24T11:54:06.542903Z","shell.execute_reply":"2024-04-24T11:54:06.581628Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Hard","metadata":{}},{"cell_type":"code","source":"#instantiate label encoders\ntask1_encoder = LabelEncoder()\n\ndef task1_hard_encode(df):\n    task1_encoder.fit(all_task1_hard_labels)\n    df['hard_label'] = task1_encoder.transform(df['hard_label'])\n    return df\n\ndef task1_hard_decode(df):\n    df[\"hard_label\"] = task1_encoder.inverse_transform(df[\"hard_label\"])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:54:08.622809Z","iopub.execute_input":"2024-04-24T11:54:08.623173Z","iopub.status.idle":"2024-04-24T11:54:08.631460Z","shell.execute_reply.started":"2024-04-24T11:54:08.623142Z","shell.execute_reply":"2024-04-24T11:54:08.630220Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"og_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\nall_task1_hard_labels = pd.concat([og_train1[\"hard_label\"], og_dev1[\"hard_label\"]])\ntrain1_df = task1_hard_encode(og_train1)\n\n# print(train1_df.columns)\ntrain1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\ntrain1_df = train1_df[train1_df['hard_label'] != 2]\ntrain1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n\ndev1_df = task1_hard_encode(og_dev1)\ndev1_df = dev1_df[[\"tweet\",\"hard_label\"]].dropna()\ndev1_df = dev1_df[dev1_df['hard_label'] != 2]\ndev1_df[\"tweet\"] = simple_preprocess(dev1_df[\"tweet\"])\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\n# test1_df = test1_df[test1_df['hard_label'] != 2]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\nprint(\"train1\",train1_df.shape)\nprint(train1_df.head)\nprint(\"test1\",test1_df.shape)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:54:11.023629Z","iopub.execute_input":"2024-04-24T11:54:11.024006Z","iopub.status.idle":"2024-04-24T11:54:11.096751Z","shell.execute_reply.started":"2024-04-24T11:54:11.023975Z","shell.execute_reply":"2024-04-24T11:54:11.095783Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"train1 (6064, 2)\n<bound method NDFrame.head of                                                   tweet  hard_label\n0     @user Ignora al otro, es un capullo.El problem...           1\n1     @user Si comicsgate se parece en algo a gamerg...           0\n2     @user Lee sobre Gamergate, y como eso ha cambi...           0\n4     @user @user @user Entonces como así es el merc...           1\n5     @user Aaah sí. Andrew Dobson. El que se dedicó...           0\n...                                                 ...         ...\n6915  idk why y’all bitches think having half your a...           1\n6916  This has been a part of an experiment with @us...           1\n6917  \"Take me already\" \"Not yet. You gotta be ready...           1\n6918       @user why do you look like a whore? /lh http           1\n6919  ik when mandy says “you look like a whore” i l...           1\n\n[6064 rows x 2 columns]>\ntest1 (2076, 1)\n<bound method NDFrame.head of                                                   tweet\n0     @user Todo gamergate desde el desarrollo hasta...\n1     @user @user Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @user Primero fue internet, luego el gamergate...\n4     @user Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @user This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @user @user You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:54:15.737911Z","iopub.execute_input":"2024-04-24T11:54:15.738280Z","iopub.status.idle":"2024-04-24T11:54:15.752616Z","shell.execute_reply.started":"2024-04-24T11:54:15.738250Z","shell.execute_reply":"2024-04-24T11:54:15.751610Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer and model\nmodel_name = \"sdadas/xlm-roberta-large-twitter\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,ignore_mismatched_sizes=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:32:44.921412Z","iopub.execute_input":"2024-04-24T12:32:44.921852Z","iopub.status.idle":"2024-04-24T12:33:02.356582Z","shell.execute_reply.started":"2024-04-24T12:32:44.921814Z","shell.execute_reply":"2024-04-24T12:33:02.355576Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:37:02.510722Z","iopub.execute_input":"2024-04-24T12:37:02.511631Z","iopub.status.idle":"2024-04-24T12:37:02.515635Z","shell.execute_reply.started":"2024-04-24T12:37:02.511594Z","shell.execute_reply":"2024-04-24T12:37:02.514695Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"tweets_dataset = DatasetDict(\n    train=train_dataset.shuffle(seed=1111),\n    val=val_dataset.shuffle(seed=1111),\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:39:34.809702Z","iopub.execute_input":"2024-04-24T12:39:34.810612Z","iopub.status.idle":"2024-04-24T12:39:34.831867Z","shell.execute_reply.started":"2024-04-24T12:39:34.810575Z","shell.execute_reply":"2024-04-24T12:39:34.830903Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def tokenize(batch):\n    print(batch)\n    return tokenizer(batch['text'], padding=True, truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:40:23.296702Z","iopub.execute_input":"2024-04-24T12:40:23.297636Z","iopub.status.idle":"2024-04-24T12:40:23.302439Z","shell.execute_reply.started":"2024-04-24T12:40:23.297597Z","shell.execute_reply":"2024-04-24T12:40:23.301458Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Tokenize the dataset\ntokenized_dataset = tweets_dataset.map(tokenize, batched=True, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:40:25.163208Z","iopub.execute_input":"2024-04-24T12:40:25.163569Z","iopub.status.idle":"2024-04-24T12:40:25.497536Z","shell.execute_reply.started":"2024-04-24T12:40:25.163539Z","shell.execute_reply":"2024-04-24T12:40:25.496046Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5948 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23878a22b6304b8fac34a077c1606e29"}},"metadata":{}},{"name":"stdout","text":"{'input_ids': [[0, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 438, 38260, 21, 60853, 115203, 16409, 67067, 849, 22, 1973, 88, 3307, 4, 2194, 35261, 51, 7926, 104, 28488, 22, 459, 8096, 166, 329, 47189, 4, 196, 110, 3332, 32116, 147, 2245, 4, 196, 110, 113876, 2245, 4, 110, 2639, 198, 28488, 146, 41783, 4, 41, 2054, 4105, 224, 170864, 31, 32, 11918, 552, 96091, 849, 88, 3128, 8, 27506, 4, 33239, 41, 110, 459, 10481, 824, 4, 21, 42648, 11, 6, 52406, 54941, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1215, 3229, 87, 5809, 959, 160093, 79442, 56, 221, 87, 1902, 47, 79442, 10, 49726, 221, 759, 55983, 217064, 2367, 87, 509, 31577, 47, 33022, 6, 115114, 1621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 6, 3, 77241, 22, 36220, 5708, 4, 45698, 1973, 13130, 65830, 10, 576, 185209, 162, 8, 576, 118684, 2701, 4, 41, 6805, 22, 65928, 196, 18602, 143829, 7, 1466, 2245, 113, 16789, 7, 5, 245811, 239719, 246392, 101460, 840, 30641, 481, 158, 31003, 596, 21, 12562, 1436, 133522, 8, 123191, 41, 1570, 131776, 85, 22, 88, 8916, 5, 14701, 284, 196, 21, 88927, 4, 6, 27488, 2701, 38, 1621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 873, 1297, 48, 5708, 576, 184940, 162, 25191, 5, 3561, 4574, 4, 10, 56752, 199, 41, 555, 19276, 38, 1250, 979, 5720, 4, 101739, 38, 239, 26897, 1570, 4, 1148, 101739, 113, 216452, 42, 38, 345, 14437, 100942, 216452, 246, 113, 18451, 4, 533, 31508, 92713, 4, 88, 1817, 25553, 79322, 95, 81255, 113, 95, 11473, 63323, 4017, 22, 2366, 8, 21652, 557, 5, 239, 211547, 2265, 108577, 11, 38, 1621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1374, 65918, 602, 370, 76737, 14324, 4, 1148, 576, 120, 1073, 113, 576, 150674, 2512, 4, 163, 182, 3290, 22334, 115114, 115114, 115114, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 21, 22, 41727, 1704, 13747, 12, 8650, 112, 146, 75013, 226719, 4, 40, 199, 3004, 192, 41, 88, 75013, 256, 842, 31580, 21, 107987, 95, 45739, 12, 110, 163, 741, 85, 21, 22, 41727, 11, 12, 110, 4, 110, 163, 513, 85, 38, 1520, 163, 513, 246, 4, 300, 163, 513, 85, 20091, 4, 40, 7828, 12632, 1621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 7680, 430, 10, 10313, 42, 10, 190, 188, 32542, 31, 158, 324, 20764, 238, 82494, 4, 59423, 4, 151, 21303, 110, 30701, 166, 73912, 18953, 3005, 21, 6, 1165, 282, 4705, 37419, 113, 1973, 88, 3307, 49512, 41, 198, 51, 51703, 6, 244683, 36611, 4, 110, 2194, 388, 72600, 6604, 24024, 840, 10, 63387, 26895, 170924, 8, 220, 22, 20088, 85, 8, 79, 52960, 113, 14726, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 30412, 51, 11544, 2704, 158, 87904, 7, 8, 9641, 13810, 113, 18451, 29990, 2194, 10, 96298, 1140, 53834, 41, 11473, 32, 25906, 90837, 42, 10, 36290, 11740, 2986, 32, 61846, 282, 8, 51, 208947, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1374, 65918, 1374, 65918, 1374, 65918, 1374, 65918, 3936, 1723, 3757, 9333, 1100, 3119, 32, 3936, 284, 83501, 55028, 13844, 21, 26137, 55970, 1150, 41, 38395, 19, 32, 239, 50815, 198, 17646, 5446, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2369, 18316, 22, 88, 54104, 142074, 8, 21, 177762, 8, 36573, 1353, 90, 27274, 8831, 7, 146, 1374, 65918, 107844, 196, 88, 22046, 20179, 8, 71774, 86, 516, 113, 74802, 3812, 5, 145111, 56, 6528, 121, 183134, 4261, 2729, 366, 3812, 4, 10, 26897, 8, 21, 224203, 4, 110, 5904, 47531, 140640, 113, 11007, 8, 92774, 21, 110245, 63, 1374, 65918, 1621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1374, 65918, 1374, 65918, 17367, 214, 5773, 32, 581, 332, 140533, 1919, 17265, 112397, 70, 111503, 5, 87, 4734, 15673, 70, 24941, 2750, 23742, 11762, 1819, 617, 39, 621, 182, 162, 5281, 237, 5045, 47, 8783, 24643, 707, 83, 6, 69986, 70, 5915, 81423, 1286, 213850, 100, 70, 22, 10023, 223, 3501, 915, 2424, 20653, 85168, 214, 2363, 180187, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 19778, 198, 1374, 65918, 198, 21, 114823, 74, 121, 41, 1658, 133, 74, 310, 20129, 330, 10, 178068, 8, 1483, 15653, 1090, 5, 6610, 191536, 5, 1657, 51, 12257, 1005, 74306, 113, 20244, 51744, 32903, 10, 5917, 46187, 3666, 113, 106076, 15467, 5, 1621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1374, 65918, 74353, 944, 4, 70, 84084, 1556, 221, 27983, 60449, 68034, 9232, 62016, 214, 111, 3395, 6044, 237, 398, 4, 1284, 642, 621, 138009, 47, 765, 398, 5, 14804, 103608, 136, 450, 111, 27060, 1884, 398, 83, 70, 7401, 3129, 2258, 3132, 16065, 70, 6, 34165, 111, 233, 48448, 450, 1379, 25720, 221, 5045, 111, 70, 5744, 15, 157225, 538, 113, 18, 16, 106820, 5, 25689, 398, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1374, 65918, 276, 39, 2276, 13042, 189781, 7, 120, 1073, 121, 220, 43281, 125618, 66419, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1374, 65918, 1184, 220, 19657, 11, 87195, 82560, 533, 166, 48947, 113, 75013, 88, 2057, 52406, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 5813, 30632, 198, 224, 208489, 42, 4, 12340, 38, 357, 10279, 8163, 163, 764, 181923, 246, 8, 220, 22334, 21089, 4, 8, 166, 14019, 4, 166, 9244, 113, 51, 160, 3181, 188, 1005, 78, 74925, 8, 21, 75013, 913, 1374, 65918, 5, 145640, 2837, 23129, 188, 8, 1973, 533, 220, 300, 5708, 58538, 4, 163, 41650, 37872, 28575, 2124, 10064, 38, 992, 70908, 113, 33587, 3879, 10, 3879, 10, 47, 71, 981, 65918, 38, 49933, 15755, 1621, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'label': [0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0]}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtweets_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:868\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 868\u001b[0m     {\n\u001b[1;32m    869\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    868\u001b[0m     {\n\u001b[0;32m--> 869\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n","Cell \u001b[0;32mIn[33], line 3\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(batch):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:270\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 270\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    272\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n","\u001b[0;31mKeyError\u001b[0m: 'text'"],"ename":"KeyError","evalue":"'text'","output_type":"error"}]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 128\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    # accelerator = Accelerator(),\n    output_dir=\"./output/best-model\",\n    report_to=None,\n    num_train_epochs=4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=5.4734446759247935e-06,\n    weight_decay=0.004881728253476856,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:42:49.058659Z","iopub.execute_input":"2024-04-24T12:42:49.059111Z","iopub.status.idle":"2024-04-24T12:42:49.109957Z","shell.execute_reply.started":"2024-04-24T12:42:49.059078Z","shell.execute_reply":"2024-04-24T12:42:49.109009Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:43:10.196023Z","iopub.execute_input":"2024-04-24T12:43:10.196422Z","iopub.status.idle":"2024-04-24T12:43:10.202177Z","shell.execute_reply.started":"2024-04-24T12:43:10.196375Z","shell.execute_reply":"2024-04-24T12:43:10.201180Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:55:16.915167Z","iopub.execute_input":"2024-04-24T11:55:16.915545Z","iopub.status.idle":"2024-04-24T11:55:18.553533Z","shell.execute_reply.started":"2024-04-24T11:55:16.915513Z","shell.execute_reply":"2024-04-24T11:55:18.552456Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def compute_f1_score(pred):\n    # pred is a tuple (predictions, labels)\n    predictions, labels = pred\n    # Compute the F1 score\n    f1 = f1_score(labels, predictions.argmax(axis=1), average='macro')\n    return {\"f1_score\": f1}","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:55:22.756028Z","iopub.execute_input":"2024-04-24T11:55:22.757036Z","iopub.status.idle":"2024-04-24T11:55:22.761775Z","shell.execute_reply.started":"2024-04-24T11:55:22.756999Z","shell.execute_reply":"2024-04-24T11:55:22.760840Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/output/best-model/*","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:22:40.873202Z","iopub.execute_input":"2024-04-24T13:22:40.873582Z","iopub.status.idle":"2024-04-24T13:22:42.725577Z","shell.execute_reply.started":"2024-04-24T13:22:40.873551Z","shell.execute_reply":"2024-04-24T13:22:42.724420Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"optimizers (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional, defaults to (None, None)) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your model and a scheduler given by get_linear_schedule_with_warmup() controlled by args.","metadata":{}},{"cell_type":"code","source":"# model.to(device)\n# Define the trainer for each fold\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_f1_score\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:43:17.952654Z","iopub.execute_input":"2024-04-24T12:43:17.953564Z","iopub.status.idle":"2024-04-24T13:12:40.563920Z","shell.execute_reply.started":"2024-04-24T12:43:17.953529Z","shell.execute_reply":"2024-04-24T13:12:40.562891Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1488' max='1488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1488/1488 29:20, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.434400</td>\n      <td>0.457243</td>\n      <td>0.853331</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.272000</td>\n      <td>0.476295</td>\n      <td>0.854624</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1488, training_loss=0.2577615809696977, metrics={'train_runtime': 1760.1613, 'train_samples_per_second': 13.517, 'train_steps_per_second': 0.845, 'total_flos': 5543127749074944.0, 'train_loss': 0.2577615809696977, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_labels = np.argmax(val_predictions.predictions, axis=1)\nval_true_labels = val_dataset[\"label\"]\n\nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels)\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:13:00.353684Z","iopub.execute_input":"2024-04-24T13:13:00.354590Z","iopub.status.idle":"2024-04-24T13:13:18.498616Z","shell.execute_reply.started":"2024-04-24T13:13:00.354551Z","shell.execute_reply":"2024-04-24T13:13:18.497770Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.8533333333333334\nValidation F1 Score: 0.8527724665391969\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions on the test set\nfrom scipy.special import softmax\ntest_predictions = trainer.predict(test_dataset)\ntest_pred_labels = np.argmax(test_predictions.predictions, axis=1)\ntest_pred_probs = softmax(test_predictions.predictions).tolist()\n\n#get probabilities to a list\ntest_probs_list = [] \nfor logits in test_pred_probs:\n    test_probs_list.append({'YES': logits[0], 'NO': logits[1]})","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:13:47.943251Z","iopub.execute_input":"2024-04-24T13:13:47.944035Z","iopub.status.idle":"2024-04-24T13:14:24.255037Z","shell.execute_reply.started":"2024-04-24T13:13:47.944001Z","shell.execute_reply":"2024-04-24T13:14:24.253962Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id\"].astype(int)\n    output_dict = {}\n    decoded_labels = task1_hard_decode(df)[\"hard_label\"].tolist()\n    print(\"decoded labels length:\",len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = decoded_labels[i]\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"./output/task1_NICA-1.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(test_pred_labels,columns=[\"hard_label\"]), test_probs_list, og_test1[[\"id\"]],og_test1)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:15:10.596248Z","iopub.execute_input":"2024-04-24T13:15:10.597100Z","iopub.status.idle":"2024-04-24T13:15:10.681713Z","shell.execute_reply.started":"2024-04-24T13:15:10.597065Z","shell.execute_reply":"2024-04-24T13:15:10.680765Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"decoded labels length: 2076\noutput generated as json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Soft","metadata":{}},{"cell_type":"code","source":"device  =  torch. device('cuda:0'  if  torch. cuda. is_available() else  'cpu')\nprint(f\"computation will run on {device} now\")","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:20:21.473106Z","iopub.execute_input":"2024-04-24T13:20:21.473549Z","iopub.status.idle":"2024-04-24T13:20:21.479104Z","shell.execute_reply.started":"2024-04-24T13:20:21.473518Z","shell.execute_reply":"2024-04-24T13:20:21.478213Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"computation will run on cuda:0 now\n","output_type":"stream"}]},{"cell_type":"code","source":"#instantiate label encoders\ndef convert_logits_to_list(logits_dict):\n    logits_dict = eval(logits_dict)\n    logits_list = [logits_dict[\"YES\"], logits_dict[\"NO\"]]\n    return logits_list\n\ndef convert_list_to_logits(logits_list):\n    logits_dict = {\"YES\": logits_list[0], \"NO\": logits_list[1]}\n    return logits_dict\n\ndef check_dtype(given_data):\n    return eval(given_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:20:25.662213Z","iopub.execute_input":"2024-04-24T13:20:25.662872Z","iopub.status.idle":"2024-04-24T13:20:25.668811Z","shell.execute_reply.started":"2024-04-24T13:20:25.662836Z","shell.execute_reply":"2024-04-24T13:20:25.667721Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/exist-clef-2024-task-1/train_data_task1.csv')\ntest_df = pd.read_csv('/kaggle/input/exist-clef-2024-task-1/EXIT2024_tweet_test.csv')\ndev_df = pd.read_csv('/kaggle/input/exist-clef-2024-task-1/dev_data_task1.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:20:28.880159Z","iopub.execute_input":"2024-04-24T13:20:28.880509Z","iopub.status.idle":"2024-04-24T13:20:29.025620Z","shell.execute_reply.started":"2024-04-24T13:20:28.880481Z","shell.execute_reply":"2024-04-24T13:20:29.024705Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"og_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\nog_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(convert_logits_to_list)\nog_train1[\"tweet\"] = simple_preprocess(og_train1[\"tweet\"])\n# og_train1[\"soft_label\"] = og_train1[\"soft_label\"].apply(check_dtype) \ntrain1_df = og_train1[[\"tweet\",\"soft_label\"]].dropna()\n\n\nog_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(convert_logits_to_list)\nog_dev1[\"tweet\"] = simple_preprocess(og_dev1[\"tweet\"])\n# og_dev1[\"soft_label\"] = og_dev1[\"soft_label\"].apply(check_dtype) \ndev1_df = og_dev1[[\"tweet\",\"soft_label\"]].dropna()\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\n# test1_df = test1_df[test1_df['hard_label'] != 2]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\nprint(\"train1\",train1_df.shape)\nprint(train1_df.head)\nprint(\"test1\",test1_df.shape)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:20:32.979881Z","iopub.execute_input":"2024-04-24T13:20:32.980241Z","iopub.status.idle":"2024-04-24T13:20:33.158370Z","shell.execute_reply.started":"2024-04-24T13:20:32.980211Z","shell.execute_reply":"2024-04-24T13:20:33.157453Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"train1 (6920, 2)\n<bound method NDFrame.head of                                                   tweet  \\\n0     @user Ignora al otro, es un capullo.El problem...   \n1     @user Si comicsgate se parece en algo a gamerg...   \n2     @user Lee sobre Gamergate, y como eso ha cambi...   \n3     @user Un retraso social bastante lamentable, g...   \n4     @user @user @user Entonces como así es el merc...   \n...                                                 ...   \n6915  idk why y’all bitches think having half your a...   \n6916  This has been a part of an experiment with @us...   \n6917  \"Take me already\" \"Not yet. You gotta be ready...   \n6918       @user why do you look like a whore? /lh http   \n6919  ik when mandy says “you look like a whore” i l...   \n\n                                     soft_label  \n0     [0.8333333333333334, 0.16666666666666666]  \n1     [0.16666666666666666, 0.8333333333333334]  \n2                                    [0.0, 1.0]  \n3                                    [0.5, 0.5]  \n4      [0.6666666666666666, 0.3333333333333333]  \n...                                         ...  \n6915                                 [1.0, 0.0]  \n6916                                 [1.0, 0.0]  \n6917   [0.6666666666666666, 0.3333333333333333]  \n6918                                 [1.0, 0.0]  \n6919  [0.8333333333333334, 0.16666666666666666]  \n\n[6920 rows x 2 columns]>\ntest1 (2076, 1)\n<bound method NDFrame.head of                                                   tweet\n0     @user Todo gamergate desde el desarrollo hasta...\n1     @user @user Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @user Primero fue internet, luego el gamergate...\n4     @user Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @user This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @user @user You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:21:15.972191Z","iopub.execute_input":"2024-04-24T13:21:15.972859Z","iopub.status.idle":"2024-04-24T13:21:15.987565Z","shell.execute_reply.started":"2024-04-24T13:21:15.972826Z","shell.execute_reply":"2024-04-24T13:21:15.986739Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer and model\nmodel_name = \"sdadas/xlm-roberta-large-twitter\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,ignore_mismatched_sizes=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:21:19.178634Z","iopub.execute_input":"2024-04-24T13:21:19.179012Z","iopub.status.idle":"2024-04-24T13:21:37.413842Z","shell.execute_reply.started":"2024-04-24T13:21:19.178981Z","shell.execute_reply":"2024-04-24T13:21:37.412902Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sdadas/xlm-roberta-large-twitter and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 128\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    # accelerator = Accelerator(),\n    output_dir=\"./output/best-model/soft/\",\n    report_to=None,\n    num_train_epochs=4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=5.4734446759247935e-06,\n    weight_decay=0.004881728253476856,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:21:48.909797Z","iopub.execute_input":"2024-04-24T13:21:48.910151Z","iopub.status.idle":"2024-04-24T13:21:48.970227Z","shell.execute_reply.started":"2024-04-24T13:21:48.910123Z","shell.execute_reply":"2024-04-24T13:21:48.969475Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\ndef convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"soft_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:21:51.753518Z","iopub.execute_input":"2024-04-24T13:21:51.753921Z","iopub.status.idle":"2024-04-24T13:21:51.760246Z","shell.execute_reply.started":"2024-04-24T13:21:51.753887Z","shell.execute_reply":"2024-04-24T13:21:51.759261Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:21:53.804230Z","iopub.execute_input":"2024-04-24T13:21:53.804627Z","iopub.status.idle":"2024-04-24T13:21:55.384053Z","shell.execute_reply.started":"2024-04-24T13:21:53.804591Z","shell.execute_reply":"2024-04-24T13:21:55.383265Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def custom_loss_fn(logits, soft_labels):\n    probs = F.softmax(logits, dim=1)\n    # Apply nn.CrossEntropyLoss\n    loss = nn.CrossEntropyLoss(reduction=\"sum\",label_smoothing=0.15)(probs, soft_labels)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:21:56.302256Z","iopub.execute_input":"2024-04-24T13:21:56.303170Z","iopub.status.idle":"2024-04-24T13:21:56.308401Z","shell.execute_reply.started":"2024-04-24T13:21:56.303132Z","shell.execute_reply":"2024-04-24T13:21:56.307437Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = custom_loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# Define the trainer for each fold\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:23:05.138321Z","iopub.execute_input":"2024-04-24T13:23:05.139121Z","iopub.status.idle":"2024-04-24T13:56:59.986408Z","shell.execute_reply.started":"2024-04-24T13:23:05.139082Z","shell.execute_reply":"2024-04-24T13:56:59.985499Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1692' max='1692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1692/1692 33:52, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>9.814700</td>\n      <td>18.764412</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>9.251600</td>\n      <td>18.801676</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>9.050900</td>\n      <td>18.728901</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1692, training_loss=9.26946732902076, metrics={'train_runtime': 2032.4739, 'train_samples_per_second': 13.312, 'train_steps_per_second': 0.832, 'total_flos': 6303583741550592.0, 'train_loss': 9.26946732902076, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"\nfrom scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_logit_labels = val_predictions.predictions\nval_pred_probabilities = softmax(val_pred_logit_labels, axis=1)\n\nval_pred_hard_labels = np.argmax(val_pred_probabilities, axis=1)\nval_true_labels = val_dataset[\"label\"]\nval_true_hard_labels = np.argmax(val_dataset[\"label\"],axis=1)\n\n# Calculate evaluation metrics for each fold\nval_accuracy = accuracy_score(val_true_hard_labels, val_pred_hard_labels)\nval_f1_score = f1_score(val_true_hard_labels, val_pred_hard_labels,average='weighted')\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:57:03.663887Z","iopub.execute_input":"2024-04-24T13:57:03.664278Z","iopub.status.idle":"2024-04-24T13:57:24.582474Z","shell.execute_reply.started":"2024-04-24T13:57:03.664237Z","shell.execute_reply":"2024-04-24T13:57:24.581577Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.830820770519263\nValidation F1 Score: 0.8307837281323904\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions on the test set\nfrom scipy.special import softmax\ntest_predictions1 = trainer.predict(test_dataset)\ntest_pred_labels1 = np.argmax(test_predictions1.predictions, axis=1)\ntest_pred_probs1 = softmax(test_predictions1.predictions).tolist()\n\n#get probabilities to a list\ntest_probs_list1 = [] \nfor logits in test_pred_probs1:\n    test_probs_list1.append({'YES': logits[0], 'NO': logits[1]})","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:57:24.584354Z","iopub.execute_input":"2024-04-24T13:57:24.584637Z","iopub.status.idle":"2024-04-24T13:58:00.314910Z","shell.execute_reply.started":"2024-04-24T13:57:24.584612Z","shell.execute_reply":"2024-04-24T13:58:00.314147Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id\"].astype(int)\n    output_dict = {}\n    decoded_labels = task1_hard_decode(df)[\"hard_label\"].tolist()\n    print(\"decoded labels length:\",len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = decoded_labels[i]\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"task1_NICA-2.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(test_pred_labels,columns=[\"hard_label\"]), test_probs_list1, og_test1[[\"id\"]],og_test1)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:58:00.316128Z","iopub.execute_input":"2024-04-24T13:58:00.316449Z","iopub.status.idle":"2024-04-24T13:58:00.400869Z","shell.execute_reply.started":"2024-04-24T13:58:00.316421Z","shell.execute_reply":"2024-04-24T13:58:00.400090Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"decoded labels length: 2076\noutput generated as json\n","output_type":"stream"}]}]}