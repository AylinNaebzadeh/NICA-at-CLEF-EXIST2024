{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8242355,"sourceType":"datasetVersion","datasetId":4889218}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-03T08:31:13.894412Z","iopub.execute_input":"2024-05-03T08:31:13.894748Z","iopub.status.idle":"2024-05-03T08:31:15.638374Z","shell.execute_reply.started":"2024-05-03T08:31:13.894721Z","shell.execute_reply":"2024-05-03T08:31:15.637302Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/train_data_task1.csv\n/kaggle/input/dev_data_task3.csv\n/kaggle/input/train_data_task2.csv\n/kaggle/input/dev_data_task1.csv\n/kaggle/input/train_data_task3.csv\n/kaggle/input/EXIT2024_tweet_test.csv\n/kaggle/input/dev_data_task2.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/output/best-model/*","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:41:30.338138Z","iopub.execute_input":"2024-05-01T06:41:30.339405Z","iopub.status.idle":"2024-05-01T06:41:31.976927Z","shell.execute_reply.started":"2024-05-01T06:41:30.339358Z","shell.execute_reply":"2024-05-01T06:41:31.975686Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb offline\n!wandb disabled","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:31:22.271920Z","iopub.execute_input":"2024-05-03T08:31:22.272780Z","iopub.status.idle":"2024-05-03T08:31:26.909805Z","shell.execute_reply.started":"2024-05-03T08:31:22.272746Z","shell.execute_reply":"2024-05-03T08:31:26.908790Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\nW&B disabled.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:31:31.130476Z","iopub.execute_input":"2024-05-03T08:31:31.131196Z","iopub.status.idle":"2024-05-03T08:31:58.828533Z","shell.execute_reply.started":"2024-05-03T08:31:31.131163Z","shell.execute_reply":"2024-05-03T08:31:58.827490Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from  IPython. display import clear_output\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport pandas as pd\nimport numpy as np\nimport random\nimport  matplotlib. pyplot  as  plt\nfrom tqdm import tqdm\n\nimport torch\nimport torch.optim as optim\nimport torch. nn.functional as F\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:31:58.831457Z","iopub.execute_input":"2024-05-03T08:31:58.831854Z","iopub.status.idle":"2024-05-03T08:32:26.801213Z","shell.execute_reply.started":"2024-05-03T08:31:58.831817Z","shell.execute_reply":"2024-05-03T08:32:26.800173Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-03 08:32:13.861903: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-03 08:32:13.862037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-03 08:32:14.118635: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/train_data_task1.csv')\ntest_df = pd.read_csv('/kaggle/input/EXIT2024_tweet_test.csv')\ndev_df = pd.read_csv('/kaggle/input/dev_data_task1.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:32:26.802488Z","iopub.execute_input":"2024-05-03T08:32:26.803307Z","iopub.status.idle":"2024-05-03T08:32:26.945208Z","shell.execute_reply.started":"2024-05-03T08:32:26.803271Z","shell.execute_reply":"2024-05-03T08:32:26.943996Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T05:47:43.504610Z","iopub.execute_input":"2024-05-01T05:47:43.504980Z","iopub.status.idle":"2024-05-01T05:47:43.546108Z","shell.execute_reply.started":"2024-05-01T05:47:43.504949Z","shell.execute_reply":"2024-05-01T05:47:43.545025Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2076 entries, 0 to 2075\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      2076 non-null   int64 \n 1   lang    2076 non-null   object\n 2   tweet   2076 non-null   object\n 3   split   2076 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 65.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T11:41:46.830848Z","iopub.execute_input":"2024-04-23T11:41:46.831499Z","iopub.status.idle":"2024-04-23T11:41:46.847794Z","shell.execute_reply.started":"2024-04-23T11:41:46.831469Z","shell.execute_reply":"2024-04-23T11:41:46.846849Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"       id lang                                              tweet    split\n0  500001   es  @Eurogamer_es Todo gamergate desde el desarrol...  TEST_ES\n1  500002   es  @ArCaNgEl__23 @Benzenazi Hombre, no es compara...  TEST_ES\n2  500003   es  yo buscando las empresas metidas en el gamerga...  TEST_ES\n3  500004   es  @jordirico Primero fue internet, luego el game...  TEST_ES\n4  500005   es  @AlonsoQuijano12 Yo estuve metido en el gamerg...  TEST_ES","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>lang</th>\n      <th>tweet</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>500001</td>\n      <td>es</td>\n      <td>@Eurogamer_es Todo gamergate desde el desarrol...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>500002</td>\n      <td>es</td>\n      <td>@ArCaNgEl__23 @Benzenazi Hombre, no es compara...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500003</td>\n      <td>es</td>\n      <td>yo buscando las empresas metidas en el gamerga...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>500004</td>\n      <td>es</td>\n      <td>@jordirico Primero fue internet, luego el game...</td>\n      <td>TEST_ES</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500005</td>\n      <td>es</td>\n      <td>@AlonsoQuijano12 Yo estuve metido en el gamerg...</td>\n      <td>TEST_ES</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def simple_preprocess(text):\n    \"\"\"\n    pass the tweet data as a series. do not use apply function\n    only preprocesses for replacing @USER and URLS\n    \"\"\"\n    URL_RE = re.compile(r\"https?:\\/\\/[\\w\\.\\/\\?\\=\\d&#%_:/-]+\")\n    HANDLE_RE = re.compile(r\"@\\w+\")\n    tweets = []\n    for t in text:\n        t = HANDLE_RE.sub(\"@user\", t)\n        t = URL_RE.sub(\"http\", t)\n        tweets.append(t)\n    return tweets","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:32:26.947831Z","iopub.execute_input":"2024-05-03T08:32:26.948244Z","iopub.status.idle":"2024-05-03T08:32:26.962900Z","shell.execute_reply.started":"2024-05-03T08:32:26.948213Z","shell.execute_reply":"2024-05-03T08:32:26.961858Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport os.path\nfrom os import path\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, TrainingArguments, Trainer\nfrom datasets import Dataset\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:32:26.964352Z","iopub.execute_input":"2024-05-03T08:32:26.964672Z","iopub.status.idle":"2024-05-03T08:32:27.018699Z","shell.execute_reply.started":"2024-05-03T08:32:26.964646Z","shell.execute_reply":"2024-05-03T08:32:27.017733Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Hard","metadata":{}},{"cell_type":"code","source":"#instantiate label encoders\ntask1_encoder = LabelEncoder()\n\ndef task1_hard_encode(df):\n    task1_encoder.fit(all_task1_hard_labels)\n    df['hard_label'] = task1_encoder.transform(df['hard_label'])\n    return df\n\ndef task1_hard_decode(df):\n    df[\"hard_label\"] = task1_encoder.inverse_transform(df[\"hard_label\"])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:32:27.020161Z","iopub.execute_input":"2024-05-03T08:32:27.020883Z","iopub.status.idle":"2024-05-03T08:32:27.028656Z","shell.execute_reply.started":"2024-05-03T08:32:27.020844Z","shell.execute_reply":"2024-05-03T08:32:27.027509Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"og_train1 = train_df.copy()\nog_dev1 = dev_df.copy()\nog_test1 = test_df.copy()\n\nall_task1_hard_labels = pd.concat([og_train1[\"hard_label\"], og_dev1[\"hard_label\"]])\ntrain1_df = task1_hard_encode(og_train1)\n\n# print(train1_df.columns)\ntrain1_df = train1_df[[\"tweet\",\"hard_label\"]].dropna()\ntrain1_df = train1_df[train1_df['hard_label'] != 2]\ntrain1_df[\"tweet\"] = simple_preprocess(train1_df[\"tweet\"])\n\ndev1_df = task1_hard_encode(og_dev1)\ndev1_df = dev1_df[[\"tweet\",\"hard_label\"]].dropna()\ndev1_df = dev1_df[dev1_df['hard_label'] != 2]\ndev1_df[\"tweet\"] = simple_preprocess(dev1_df[\"tweet\"])\n\ntest1_df = og_test1\ntest1_df = test1_df[[\"tweet\"]]\n# test1_df = test1_df[test1_df['hard_label'] != 2]\ntest1_df[\"tweet\"] = simple_preprocess(test1_df[\"tweet\"])\n\nprint(\"train1\",train1_df.shape)\nprint(train1_df.head)\nprint(\"test1\",test1_df.shape)\nprint(test1_df.head)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:32:27.030370Z","iopub.execute_input":"2024-05-03T08:32:27.031127Z","iopub.status.idle":"2024-05-03T08:32:27.108280Z","shell.execute_reply.started":"2024-05-03T08:32:27.031092Z","shell.execute_reply":"2024-05-03T08:32:27.107056Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"train1 (6064, 2)\n<bound method NDFrame.head of                                                   tweet  hard_label\n0     @user Ignora al otro, es un capullo.El problem...           1\n1     @user Si comicsgate se parece en algo a gamerg...           0\n2     @user Lee sobre Gamergate, y como eso ha cambi...           0\n4     @user @user @user Entonces como así es el merc...           1\n5     @user Aaah sí. Andrew Dobson. El que se dedicó...           0\n...                                                 ...         ...\n6915  idk why y’all bitches think having half your a...           1\n6916  This has been a part of an experiment with @us...           1\n6917  \"Take me already\" \"Not yet. You gotta be ready...           1\n6918       @user why do you look like a whore? /lh http           1\n6919  ik when mandy says “you look like a whore” i l...           1\n\n[6064 rows x 2 columns]>\ntest1 (2076, 1)\n<bound method NDFrame.head of                                                   tweet\n0     @user Todo gamergate desde el desarrollo hasta...\n1     @user @user Hombre, no es comparable, mira lo ...\n2     yo buscando las empresas metidas en el gamerga...\n3     @user Primero fue internet, luego el gamergate...\n4     @user Yo estuve metido en el gamergate, asi qu...\n...                                                 ...\n2071  @user This straight up sounds like “you look l...\n2072  Nathaniel is trying to help me with a new fake...\n2073  walkin back from the gym &amp; an older lady s...\n2074  You look like a whore of Babylon bc that’s the...\n2075  @user @user You look like a whore. Stop projec...\n\n[2076 rows x 1 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Combine train1df and test1df into a single dataframe\ncombined_df = pd.concat([train1_df, dev1_df], ignore_index=True)\n\n# Shuffle the combined dataframe\ncombined_df_shuffled = combined_df.sample(frac=1, random_state=42)\n\n# Split the shuffled dataframe into train, validation, and test dataframes with an 80-10-10 split\ntrain_df, val_df = train_test_split(combined_df_shuffled, test_size=0.15, random_state=42)\ntest_df = test1_df\n\n# Reset the indices of the dataframes\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:32:27.109753Z","iopub.execute_input":"2024-05-03T08:32:27.110225Z","iopub.status.idle":"2024-05-03T08:32:27.126447Z","shell.execute_reply.started":"2024-05-03T08:32:27.110188Z","shell.execute_reply":"2024-05-03T08:32:27.125348Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# with XLMB","metadata":{}},{"cell_type":"code","source":"device  =  torch. device('cuda:0'  if  torch. cuda. is_available() else  'cpu')\nprint(f\"computation will run on {device} now\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:32:27.128023Z","iopub.execute_input":"2024-05-03T08:32:27.128417Z","iopub.status.idle":"2024-05-03T08:32:27.188126Z","shell.execute_reply.started":"2024-05-03T08:32:27.128382Z","shell.execute_reply":"2024-05-03T08:32:27.187055Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"computation will run on cuda:0 now\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')\nmodel = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-base\", num_labels=2,ignore_mismatched_sizes=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:45:26.350102Z","iopub.execute_input":"2024-05-03T08:45:26.350476Z","iopub.status.idle":"2024-05-03T08:45:38.816346Z","shell.execute_reply.started":"2024-05-03T08:45:26.350447Z","shell.execute_reply":"2024-05-03T08:45:38.815298Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673884b3929a475fa55d094ac5a59f2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5053d560bf7f4024ba749fb3f03cfb3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d993206c0284fedab937304fb9d912c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4faf10f36e9412ca367f1c0e5b452a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddca02f486754c488445839767b2a040"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:33:24.282808Z","iopub.execute_input":"2024-05-03T08:33:24.283461Z","iopub.status.idle":"2024-05-03T08:33:24.419942Z","shell.execute_reply.started":"2024-05-03T08:33:24.283427Z","shell.execute_reply":"2024-05-03T08:33:24.418695Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Training arguments for GPT large","metadata":{}},{"cell_type":"markdown","source":"https://datascience.stackexchange.com/questions/64583/what-are-the-good-parameter-ranges-for-bert-hyperparameters-while-finetuning-it","metadata":{}},{"cell_type":"code","source":"# Set parameters\nMAX_LENGTH = 512\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./output/best-model\",\n    report_to=None,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 1,\n    logging_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:47:55.040631Z","iopub.execute_input":"2024-05-03T08:47:55.041321Z","iopub.status.idle":"2024-05-03T08:47:55.074989Z","shell.execute_reply.started":"2024-05-03T08:47:55.041287Z","shell.execute_reply":"2024-05-03T08:47:55.074189Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def convert_to_dataset(df):\n    df = {\"text\": df['tweet'].tolist(), \"label\": df[\"hard_label\"].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset\n\ndef convert_to_dataset_test(df):\n    df = {\"text\": df['tweet'].tolist()}\n    dataset = Dataset.from_dict(df)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:33:44.107953Z","iopub.execute_input":"2024-05-03T08:33:44.108615Z","iopub.status.idle":"2024-05-03T08:33:44.114208Z","shell.execute_reply.started":"2024-05-03T08:33:44.108577Z","shell.execute_reply":"2024-05-03T08:33:44.113213Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Convert dataframe to dataset\ntrain_dataset = convert_to_dataset(train_df)\nval_dataset = convert_to_dataset(val_df)\ntest_dataset = convert_to_dataset_test(test_df)\n\n# Create the datasets\ntrain_encodings = tokenizer(train_dataset[\"text\"],return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LENGTH)\ntrain_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"], \"label\": train_dataset[\"label\"]})\n\nval_encodings = tokenizer(val_dataset[\"text\"], padding='max_length', truncation=True, max_length=MAX_LENGTH)\nval_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"], \"label\": val_dataset[\"label\"]})\n\n\ntest_encodings = tokenizer(test_dataset[\"text\"], padding='max_length', truncation=True, max_length=MAX_LENGTH)\ntest_dataset = Dataset.from_dict({\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:47:59.868431Z","iopub.execute_input":"2024-05-03T08:47:59.869345Z","iopub.status.idle":"2024-05-03T08:48:04.059184Z","shell.execute_reply.started":"2024-05-03T08:47:59.869310Z","shell.execute_reply":"2024-05-03T08:48:04.058125Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def compute_f1_score(pred):\n    # pred is a tuple (predictions, labels)\n    predictions, labels = pred\n    # Compute the F1 score\n    f1 = f1_score(labels, predictions.argmax(axis=1), average='macro')\n    return {\"f1_score\": f1}","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:33:51.852699Z","iopub.execute_input":"2024-05-03T08:33:51.853440Z","iopub.status.idle":"2024-05-03T08:33:51.858181Z","shell.execute_reply.started":"2024-05-03T08:33:51.853409Z","shell.execute_reply":"2024-05-03T08:33:51.857240Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"optimizers (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional, defaults to (None, None)) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your model and a scheduler given by get_linear_schedule_with_warmup() controlled by args.","metadata":{}},{"cell_type":"markdown","source":"# with lr=2e-5","metadata":{}},{"cell_type":"code","source":"# model.to(device)\n# Define the trainer for each fold\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_f1_score\n)\n\n# Train the model for each fold\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:48:10.948946Z","iopub.execute_input":"2024-05-03T08:48:10.949313Z","iopub.status.idle":"2024-05-03T09:20:57.717557Z","shell.execute_reply.started":"2024-05-03T08:48:10.949287Z","shell.execute_reply":"2024-05-03T09:20:57.716371Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1860' max='1860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1860/1860 32:43, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.519100</td>\n      <td>0.427689</td>\n      <td>0.822765</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.370700</td>\n      <td>0.449140</td>\n      <td>0.836119</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.200700</td>\n      <td>0.560502</td>\n      <td>0.836953</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1860, training_loss=0.30053384637319913, metrics={'train_runtime': 1965.0157, 'train_samples_per_second': 15.135, 'train_steps_per_second': 0.947, 'total_flos': 7824922786406400.0, 'train_loss': 0.30053384637319913, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# with lr=2e-5","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Get predictions on the validation set\nval_predictions = trainer.predict(val_dataset)\nval_pred_labels = np.argmax(val_predictions.predictions, axis=1)\nval_true_labels = val_dataset[\"label\"]\n\nval_accuracy = accuracy_score(val_true_labels, val_pred_labels)\nval_f1_score = f1_score(val_true_labels, val_pred_labels)\n\nprint(\"Validation Accuracy:\", val_accuracy)\nprint(\"Validation F1 Score:\", val_f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:22:04.821463Z","iopub.execute_input":"2024-05-03T09:22:04.821857Z","iopub.status.idle":"2024-05-03T09:22:27.277226Z","shell.execute_reply.started":"2024-05-03T09:22:04.821820Z","shell.execute_reply":"2024-05-03T09:22:27.276222Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation Accuracy: 0.8228571428571428\nValidation F1 Score: 0.8268156424581005\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get predictions on the test set\nfrom scipy.special import softmax\ntest_predictions = trainer.predict(test_dataset)\ntest_pred_labels = np.argmax(test_predictions.predictions, axis=1)\ntest_pred_probs = softmax(test_predictions.predictions).tolist()\n\n#get probabilities to a list\ntest_probs_list = [] \nfor logits in test_pred_probs:\n    test_probs_list.append({'YES': logits[0], 'NO': logits[1]})","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:22:27.278685Z","iopub.execute_input":"2024-05-03T09:22:27.279118Z","iopub.status.idle":"2024-05-03T09:23:12.019801Z","shell.execute_reply.started":"2024-05-03T09:22:27.279078Z","shell.execute_reply":"2024-05-03T09:23:12.018699Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef get_json_for_evaluation(df, probs_list, ids, gold):\n    gold[\"index\"] = gold[\"id\"].astype(int)\n    output_dict = {}\n    decoded_labels = task1_hard_decode(df)[\"hard_label\"].tolist()\n    print(\"decoded labels length:\",len(decoded_labels))\n    for i, row in enumerate(probs_list):\n        soft_label = row\n        hard_label = decoded_labels[i]\n        local_dict = {\"hard_label\":hard_label,\"soft_label\": soft_label}\n        output_dict[int(gold[\"index\"][i])] = local_dict\n    \n    filename = \"./output/task1_NICA-hard-8.json\"\n    print(\"output generated as json\")\n    with open(filename, 'w') as file:\n        json.dump(output_dict, file, indent=4)\n    return output_dict\n\noutputs_json = get_json_for_evaluation(pd.DataFrame(test_pred_labels,columns=[\"hard_label\"]), test_probs_list, og_test1[[\"id\"]],og_test1)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T09:23:12.021368Z","iopub.execute_input":"2024-05-03T09:23:12.022255Z","iopub.status.idle":"2024-05-03T09:23:12.101482Z","shell.execute_reply.started":"2024-05-03T09:23:12.022219Z","shell.execute_reply":"2024-05-03T09:23:12.100389Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"decoded labels length: 2076\noutput generated as json\n","output_type":"stream"}]}]}